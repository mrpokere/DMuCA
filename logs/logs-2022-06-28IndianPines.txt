creating ./logs/logs-2022-06-28IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-06-28:17:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt0.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt0.npy)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [1/15]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [1/15]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the Model_by_COA model
Train dataloader:30
Validation dataloader:29
----------Training parameters----------
spe_kerner:9
spa_kerner:5
dataset:IndianPines
model:Model_by_COA
comb:parallel
state:sta_dy
blocks:2
spa_head:16
spe_head:25
folder:../dataset/
cuda:0
run:15
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f2820c9b0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/100    avg_loss:2.54, val_acc:0.58]
Epoch [2/100    avg_loss:1.23, val_acc:0.63]
Epoch [3/100    avg_loss:0.75, val_acc:0.84]
Epoch [4/100    avg_loss:0.46, val_acc:0.86]
Epoch [5/100    avg_loss:0.35, val_acc:0.90]
Epoch [6/100    avg_loss:0.29, val_acc:0.88]
Epoch [7/100    avg_loss:0.22, val_acc:0.94]
Epoch [8/100    avg_loss:0.16, val_acc:0.90]
Epoch [9/100    avg_loss:0.12, val_acc:0.95]
Epoch [10/100    avg_loss:0.12, val_acc:0.95]
Epoch [11/100    avg_loss:0.07, val_acc:0.95]
Epoch [12/100    avg_loss:0.07, val_acc:0.93]
Epoch [13/100    avg_loss:0.11, val_acc:0.95]
Epoch [14/100    avg_loss:0.11, val_acc:0.92]
Epoch [15/100    avg_loss:0.11, val_acc:0.93]
Epoch [16/100    avg_loss:0.06, val_acc:0.96]
Epoch [17/100    avg_loss:0.05, val_acc:0.96]
Epoch [18/100    avg_loss:0.04, val_acc:0.96]
Epoch [19/100    avg_loss:0.02, val_acc:0.97]
Epoch [20/100    avg_loss:0.02, val_acc:0.97]
Epoch [21/100    avg_loss:0.02, val_acc:0.97]
Epoch [22/100    avg_loss:0.01, val_acc:0.97]
Epoch [23/100    avg_loss:0.01, val_acc:0.97]
Epoch [24/100    avg_loss:0.00, val_acc:0.98]
Epoch [25/100    avg_loss:0.01, val_acc:0.97]
Epoch [26/100    avg_loss:0.07, val_acc:0.96]
Epoch [27/100    avg_loss:0.04, val_acc:0.97]
Epoch [28/100    avg_loss:0.02, val_acc:0.98]
Epoch [29/100    avg_loss:0.01, val_acc:0.98]
Epoch [30/100    avg_loss:0.01, val_acc:0.98]
Epoch [31/100    avg_loss:0.00, val_acc:0.98]
Epoch [32/100    avg_loss:0.01, val_acc:0.98]
Epoch [33/100    avg_loss:0.01, val_acc:0.98]
Epoch [34/100    avg_loss:0.00, val_acc:0.98]
Epoch [35/100    avg_loss:0.01, val_acc:0.98]
Epoch [36/100    avg_loss:0.01, val_acc:0.98]
Epoch [37/100    avg_loss:0.01, val_acc:0.98]
Epoch [38/100    avg_loss:0.01, val_acc:0.98]
Epoch [39/100    avg_loss:0.00, val_acc:0.98]
Epoch [40/100    avg_loss:0.00, val_acc:0.98]
Epoch [41/100    avg_loss:0.00, val_acc:0.98]
Epoch [42/100    avg_loss:0.00, val_acc:0.98]
Epoch [43/100    avg_loss:0.00, val_acc:0.98]
Epoch [44/100    avg_loss:0.00, val_acc:0.98]
Epoch [45/100    avg_loss:0.00, val_acc:0.98]
Epoch [46/100    avg_loss:0.00, val_acc:0.98]
Epoch [47/100    avg_loss:0.00, val_acc:0.98]
Epoch [48/100    avg_loss:0.00, val_acc:0.98]
Epoch [49/100    avg_loss:0.00, val_acc:0.98]
Epoch [50/100    avg_loss:0.00, val_acc:0.98]
Epoch [51/100    avg_loss:0.00, val_acc:0.98]
Epoch [52/100    avg_loss:0.00, val_acc:0.98]
Epoch [53/100    avg_loss:0.00, val_acc:0.98]
Epoch [54/100    avg_loss:0.00, val_acc:0.98]
Epoch [55/100    avg_loss:0.00, val_acc:0.98]
Epoch [56/100    avg_loss:0.00, val_acc:0.98]
Epoch [57/100    avg_loss:0.00, val_acc:0.98]
Epoch [58/100    avg_loss:0.00, val_acc:0.98]
Epoch [59/100    avg_loss:0.00, val_acc:0.98]
Epoch [60/100    avg_loss:0.00, val_acc:0.98]
Epoch [61/100    avg_loss:0.00, val_acc:0.98]
Epoch [62/100    avg_loss:0.00, val_acc:0.98]
Epoch [63/100    avg_loss:0.00, val_acc:0.98]
Epoch [64/100    avg_loss:0.00, val_acc:0.98]
Epoch [65/100    avg_loss:0.00, val_acc:0.98]
Epoch [66/100    avg_loss:0.00, val_acc:0.98]
Epoch [67/100    avg_loss:0.00, val_acc:0.98]
Epoch [68/100    avg_loss:0.00, val_acc:0.98]
Epoch [69/100    avg_loss:0.00, val_acc:0.98]
Epoch [70/100    avg_loss:0.00, val_acc:0.98]
Epoch [71/100    avg_loss:0.00, val_acc:0.98]
Epoch [72/100    avg_loss:0.00, val_acc:0.98]
Epoch [73/100    avg_loss:0.00, val_acc:0.98]
Epoch [74/100    avg_loss:0.00, val_acc:0.98]
Epoch [75/100    avg_loss:0.00, val_acc:0.98]
Epoch [76/100    avg_loss:0.00, val_acc:0.98]
Epoch [77/100    avg_loss:0.00, val_acc:0.98]
Epoch [78/100    avg_loss:0.00, val_acc:0.98]
Epoch [79/100    avg_loss:0.00, val_acc:0.98]
Epoch [80/100    avg_loss:0.00, val_acc:0.98]
Epoch [81/100    avg_loss:0.00, val_acc:0.98]
Epoch [82/100    avg_loss:0.00, val_acc:0.98]
Epoch [83/100    avg_loss:0.00, val_acc:0.98]
Epoch [84/100    avg_loss:0.00, val_acc:0.98]
Epoch [85/100    avg_loss:0.00, val_acc:0.98]
Epoch [86/100    avg_loss:0.00, val_acc:0.98]
Epoch [87/100    avg_loss:0.00, val_acc:0.98]
Epoch [88/100    avg_loss:0.00, val_acc:0.99]
Epoch [89/100    avg_loss:0.00, val_acc:0.99]
Epoch [90/100    avg_loss:0.00, val_acc:0.99]
Epoch [91/100    avg_loss:0.00, val_acc:0.98]
Epoch [92/100    avg_loss:0.00, val_acc:0.98]
Epoch [93/100    avg_loss:0.00, val_acc:0.99]
Epoch [94/100    avg_loss:0.00, val_acc:0.98]
Epoch [95/100    avg_loss:0.00, val_acc:0.98]
Epoch [96/100    avg_loss:0.00, val_acc:0.98]
Epoch [97/100    avg_loss:0.00, val_acc:0.99]
Epoch [98/100    avg_loss:0.00, val_acc:0.98]
Epoch [99/100    avg_loss:0.00, val_acc:0.98]
Epoch [100/100    avg_loss:0.00, val_acc:0.98]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    5    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1272    1    0    0    5    0    0    0    0    7    0    0
     0    0    0]
 [   0    0    4  729    2    0    0    0    0    0    0    7    3    2
     0    0    0]
 [   0    0    0    0  210    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    0    4    0  418    1    1    0    0    1    0    0    0
    10    0    0]
 [   0    0    0    0    3    0  653    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    1    3   21    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    5    0    0    3    0    0   10    0    0    0    0
     0    0    0]
 [   0    0    2    1    0    1    0    0    0    0  855    7    5    0
     4    0    0]
 [   0    0   10    0    0    1    1    0    0    0    6 2190    0    0
     2    0    0]
 [   0    0    0   13    0    1    0    0    0    0    5    2  512    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0   34    0    0    0    0    0    0    0
    72  241    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    7    0
     0    0   77]]

Accuracy:
97.3225

F1 scores:
[   nan 0.9351 0.9868 0.972  0.9813 0.9755 0.9624 0.8936 1.     0.7143
 0.9816 0.9903 0.9624 0.9946 0.9624 0.8197 0.9506]

Kappa:
0.9694
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt1.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt1.npy)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [2/15]
RUN:1
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [2/15]
RUN:1
1024 samples selected for validation(over 10249)
Running an experiment with the Model_by_COA model
Train dataloader:30
Validation dataloader:29
----------Training parameters----------
spe_kerner:9
spa_kerner:5
dataset:IndianPines
model:Model_by_COA
comb:parallel
state:sta_dy
blocks:2
spa_head:16
spe_head:25
folder:../dataset/
cuda:0
run:15
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f2820c9b0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/100    avg_loss:4.81, val_acc:0.40]
Epoch [2/100    avg_loss:1.87, val_acc:0.55]
Epoch [3/100    avg_loss:1.27, val_acc:0.53]
Epoch [4/100    avg_loss:1.11, val_acc:0.57]
Epoch [5/100    avg_loss:0.82, val_acc:0.65]
Epoch [6/100    avg_loss:0.75, val_acc:0.64]
Epoch [7/100    avg_loss:0.55, val_acc:0.63]
Epoch [8/100    avg_loss:0.46, val_acc:0.75]
Epoch [9/100    avg_loss:0.36, val_acc:0.79]
Epoch [10/100    avg_loss:0.28, val_acc:0.89]
Epoch [11/100    avg_loss:0.23, val_acc:0.86]
Epoch [12/100    avg_loss:0.35, val_acc:0.85]
Epoch [13/100    avg_loss:0.24, val_acc:0.83]
Epoch [14/100    avg_loss:0.20, val_acc:0.85]
Epoch [15/100    avg_loss:0.19, val_acc:0.86]
Epoch [16/100    avg_loss:0.13, val_acc:0.89]
Epoch [17/100    avg_loss:0.09, val_acc:0.91]
Epoch [18/100    avg_loss:0.05, val_acc:0.89]
Epoch [19/100    avg_loss:0.08, val_acc:0.90]
Epoch [20/100    avg_loss:0.09, val_acc:0.89]
Epoch [21/100    avg_loss:0.05, val_acc:0.93]
Epoch [22/100    avg_loss:0.03, val_acc:0.94]
Epoch [23/100    avg_loss:0.04, val_acc:0.93]
Epoch [24/100    avg_loss:0.03, val_acc:0.95]
Epoch [25/100    avg_loss:0.04, val_acc:0.93]
Epoch [26/100    avg_loss:0.08, val_acc:0.93]
Epoch [27/100    avg_loss:0.12, val_acc:0.88]
Epoch [28/100    avg_loss:0.14, val_acc:0.91]
Epoch [29/100    avg_loss:0.12, val_acc:0.91]
Epoch [30/100    avg_loss:0.10, val_acc:0.91]
Epoch [31/100    avg_loss:0.08, val_acc:0.93]
Epoch [32/100    avg_loss:0.04, val_acc:0.95]
Epoch [33/100    avg_loss:0.03, val_acc:0.94]
Epoch [34/100    avg_loss:0.02, val_acc:0.95]
Epoch [35/100    avg_loss:0.01, val_acc:0.95]
Epoch [36/100    avg_loss:0.01, val_acc:0.96]
Epoch [37/100    avg_loss:0.03, val_acc:0.96]
Epoch [38/100    avg_loss:0.01, val_acc:0.96]
Epoch [39/100    avg_loss:0.01, val_acc:0.96]
Epoch [40/100    avg_loss:0.01, val_acc:0.96]
Epoch [41/100    avg_loss:0.01, val_acc:0.96]
Epoch [42/100    avg_loss:0.01, val_acc:0.96]
Epoch [43/100    avg_loss:0.01, val_acc:0.96]
Epoch [44/100    avg_loss:0.01, val_acc:0.96]
Epoch [45/100    avg_loss:0.01, val_acc:0.97]
Epoch [46/100    avg_loss:0.01, val_acc:0.96]
Epoch [47/100    avg_loss:0.01, val_acc:0.92]
Epoch [48/100    avg_loss:0.03, val_acc:0.92]
Epoch [49/100    avg_loss:0.01, val_acc:0.96]
Epoch [50/100    avg_loss:0.01, val_acc:0.96]
Epoch [51/100    avg_loss:0.01, val_acc:0.96]
Epoch [52/100    avg_loss:0.01, val_acc:0.96]
Epoch [53/100    avg_loss:0.01, val_acc:0.96]
Epoch [54/100    avg_loss:0.01, val_acc:0.97]
Epoch [55/100    avg_loss:0.00, val_acc:0.97]
Epoch [56/100    avg_loss:0.01, val_acc:0.97]
Epoch [57/100    avg_loss:0.01, val_acc:0.96]
Epoch [58/100    avg_loss:0.01, val_acc:0.96]
Epoch [59/100    avg_loss:0.01, val_acc:0.96]
Epoch [60/100    avg_loss:0.01, val_acc:0.96]
Epoch [61/100    avg_loss:0.00, val_acc:0.96]
Epoch [62/100    avg_loss:0.01, val_acc:0.96]
Epoch [63/100    avg_loss:0.00, val_acc:0.96]
Epoch [64/100    avg_loss:0.00, val_acc:0.96]
Epoch [65/100    avg_loss:0.01, val_acc:0.96]
Epoch [66/100    avg_loss:0.00, val_acc:0.96]
Epoch [67/100    avg_loss:0.00, val_acc:0.97]
Epoch [68/100    avg_loss:0.00, val_acc:0.97]
Epoch [69/100    avg_loss:0.01, val_acc:0.97]
Epoch [70/100    avg_loss:0.01, val_acc:0.96]
Epoch [71/100    avg_loss:0.00, val_acc:0.97]
Epoch [72/100    avg_loss:0.00, val_acc:0.97]
Epoch [73/100    avg_loss:0.00, val_acc:0.97]
Epoch [74/100    avg_loss:0.00, val_acc:0.97]
Epoch [75/100    avg_loss:0.00, val_acc:0.97]
Epoch [76/100    avg_loss:0.00, val_acc:0.97]
Epoch [77/100    avg_loss:0.00, val_acc:0.97]
Epoch [78/100    avg_loss:0.00, val_acc:0.97]
Epoch [79/100    avg_loss:0.00, val_acc:0.96]
Epoch [80/100    avg_loss:0.01, val_acc:0.96]
Epoch [81/100    avg_loss:0.00, val_acc:0.96]
Epoch [82/100    avg_loss:0.00, val_acc:0.97]
Epoch [83/100    avg_loss:0.00, val_acc:0.97]
Epoch [84/100    avg_loss:0.00, val_acc:0.97]
Epoch [85/100    avg_loss:0.01, val_acc:0.97]
Epoch [86/100    avg_loss:0.00, val_acc:0.97]
Epoch [87/100    avg_loss:0.00, val_acc:0.97]
Epoch [88/100    avg_loss:0.00, val_acc:0.97]
Epoch [89/100    avg_loss:0.00, val_acc:0.97]
Epoch [90/100    avg_loss:0.00, val_acc:0.97]
Epoch [91/100    avg_loss:0.00, val_acc:0.97]
Epoch [92/100    avg_loss:0.01, val_acc:0.96]
Epoch [93/100    avg_loss:0.01, val_acc:0.97]
Epoch [94/100    avg_loss:0.03, val_acc:0.94]
Epoch [95/100    avg_loss:0.02, val_acc:0.96]
Epoch [96/100    avg_loss:0.02, val_acc:0.97]
Epoch [97/100    avg_loss:0.00, val_acc:0.97]
Epoch [98/100    avg_loss:0.00, val_acc:0.96]
Epoch [99/100    avg_loss:0.04, val_acc:0.96]
Epoch [100/100    avg_loss:0.01, val_acc:0.96]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1229    0    0    0    0    0    0    0   16   38    0    0
     1    1    0]
 [   0    0   23  704    0    2    0    0    0    0    0    0   15    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    1    0    0    3    0    0    5
     0    0    0]
 [   0    0    0    0    0    0  645    0    0    0    0    0    0    2
    10    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   14    0    0    0    2
     0    0    0]
 [   0    0   17    0    0    0    0    0    0    0  819   19    5    0
     8    7    0]
 [   0    0   21    6    0    0    0    0    0    0    3 2178    0    2
     0    0    0]
 [   0    0    1    6    6    0    0    0    0    0   12    0  503    0
     0    2    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1126   13    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    83  264    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.2927

F1 scores:
[   nan 0.975  0.9542 0.9611 0.9861 0.9873 0.9908 0.9804 1.     0.875
 0.9468 0.98   0.9509 0.9635 0.9514 0.8328 0.9708]

Kappa:
0.9577
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt2.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt2.npy)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [3/15]
RUN:2
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [3/15]
RUN:2
1024 samples selected for validation(over 10249)
Running an experiment with the Model_by_COA model
Train dataloader:30
Validation dataloader:29
----------Training parameters----------
spe_kerner:9
spa_kerner:5
dataset:IndianPines
model:Model_by_COA
comb:parallel
state:sta_dy
blocks:2
spa_head:16
spe_head:25
folder:../dataset/
cuda:0
run:15
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f2820c9b0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/100    avg_loss:3.67, val_acc:0.30]
Epoch [2/100    avg_loss:2.08, val_acc:0.51]
Epoch [3/100    avg_loss:1.31, val_acc:0.42]
Epoch [4/100    avg_loss:1.34, val_acc:0.65]
Epoch [5/100    avg_loss:0.81, val_acc:0.61]
Epoch [6/100    avg_loss:0.83, val_acc:0.59]
Epoch [7/100    avg_loss:0.71, val_acc:0.69]
Epoch [8/100    avg_loss:0.41, val_acc:0.79]
Epoch [9/100    avg_loss:0.30, val_acc:0.82]
Epoch [10/100    avg_loss:0.27, val_acc:0.80]
Epoch [11/100    avg_loss:0.34, val_acc:0.82]
Epoch [12/100    avg_loss:0.28, val_acc:0.85]
Epoch [13/100    avg_loss:0.17, val_acc:0.88]
Epoch [14/100    avg_loss:0.24, val_acc:0.81]
Epoch [15/100    avg_loss:0.28, val_acc:0.87]
Epoch [16/100    avg_loss:0.13, val_acc:0.92]
Epoch [17/100    avg_loss:0.13, val_acc:0.92]
Epoch [18/100    avg_loss:0.07, val_acc:0.93]
Epoch [19/100    avg_loss:0.08, val_acc:0.92]
Epoch [20/100    avg_loss:0.07, val_acc:0.93]
Epoch [21/100    avg_loss:0.07, val_acc:0.94]
Epoch [22/100    avg_loss:0.08, val_acc:0.95]
Epoch [23/100    avg_loss:0.05, val_acc:0.95]
Epoch [24/100    avg_loss:0.04, val_acc:0.95]
Epoch [25/100    avg_loss:0.04, val_acc:0.96]
Epoch [26/100    avg_loss:0.04, val_acc:0.96]
Epoch [27/100    avg_loss:0.02, val_acc:0.96]
Epoch [28/100    avg_loss:0.03, val_acc:0.96]
Epoch [29/100    avg_loss:0.02, val_acc:0.93]
Epoch [30/100    avg_loss:0.03, val_acc:0.96]
Epoch [31/100    avg_loss:0.02, val_acc:0.96]
Epoch [32/100    avg_loss:0.02, val_acc:0.96]
Epoch [33/100    avg_loss:0.02, val_acc:0.96]
Epoch [34/100    avg_loss:0.03, val_acc:0.96]
Epoch [35/100    avg_loss:0.02, val_acc:0.97]
Epoch [36/100    avg_loss:0.03, val_acc:0.95]
Epoch [37/100    avg_loss:0.02, val_acc:0.97]
Epoch [38/100    avg_loss:0.01, val_acc:0.96]
Epoch [39/100    avg_loss:0.01, val_acc:0.96]
Epoch [40/100    avg_loss:0.01, val_acc:0.96]
Epoch [41/100    avg_loss:0.01, val_acc:0.97]
Epoch [42/100    avg_loss:0.02, val_acc:0.95]
Epoch [43/100    avg_loss:0.03, val_acc:0.94]
Epoch [44/100    avg_loss:0.02, val_acc:0.96]
Epoch [45/100    avg_loss:0.01, val_acc:0.95]
Epoch [46/100    avg_loss:0.02, val_acc:0.96]
Epoch [47/100    avg_loss:0.01, val_acc:0.96]
Epoch [48/100    avg_loss:0.01, val_acc:0.97]
Epoch [49/100    avg_loss:0.01, val_acc:0.96]
Epoch [50/100    avg_loss:0.01, val_acc:0.97]
Epoch [51/100    avg_loss:0.02, val_acc:0.96]
Epoch [52/100    avg_loss:0.01, val_acc:0.91]
Epoch [53/100    avg_loss:0.02, val_acc:0.95]
Epoch [54/100    avg_loss:0.01, val_acc:0.96]
Epoch [55/100    avg_loss:0.01, val_acc:0.97]
Epoch [56/100    avg_loss:0.01, val_acc:0.97]
Epoch [57/100    avg_loss:0.01, val_acc:0.97]
Epoch [58/100    avg_loss:0.01, val_acc:0.96]
Epoch [59/100    avg_loss:0.02, val_acc:0.97]
Epoch [60/100    avg_loss:0.01, val_acc:0.97]
Epoch [61/100    avg_loss:0.01, val_acc:0.97]
Epoch [62/100    avg_loss:0.01, val_acc:0.97]
Epoch [63/100    avg_loss:0.01, val_acc:0.97]
Epoch [64/100    avg_loss:0.00, val_acc:0.97]
Epoch [65/100    avg_loss:0.01, val_acc:0.96]
Epoch [66/100    avg_loss:0.03, val_acc:0.96]
Epoch [67/100    avg_loss:0.02, val_acc:0.97]
Epoch [68/100    avg_loss:0.01, val_acc:0.96]
Epoch [69/100    avg_loss:0.01, val_acc:0.97]
Epoch [70/100    avg_loss:0.01, val_acc:0.96]
Epoch [71/100    avg_loss:0.01, val_acc:0.97]
Epoch [72/100    avg_loss:0.01, val_acc:0.97]
Epoch [73/100    avg_loss:0.01, val_acc:0.97]
Epoch [74/100    avg_loss:0.02, val_acc:0.97]
Epoch [75/100    avg_loss:0.01, val_acc:0.97]
Epoch [76/100    avg_loss:0.00, val_acc:0.97]
Epoch [77/100    avg_loss:0.01, val_acc:0.97]
Epoch [78/100    avg_loss:0.00, val_acc:0.97]
Epoch [79/100    avg_loss:0.00, val_acc:0.97]
Epoch [80/100    avg_loss:0.04, val_acc:0.94]
Epoch [81/100    avg_loss:0.05, val_acc:0.94]
Epoch [82/100    avg_loss:0.03, val_acc:0.97]
Epoch [83/100    avg_loss:0.01, val_acc:0.98]
Epoch [84/100    avg_loss:0.01, val_acc:0.98]
Epoch [85/100    avg_loss:0.01, val_acc:0.97]
Epoch [86/100    avg_loss:0.01, val_acc:0.98]
Epoch [87/100    avg_loss:0.00, val_acc:0.98]
Epoch [88/100    avg_loss:0.01, val_acc:0.97]
Epoch [89/100    avg_loss:0.01, val_acc:0.98]
Epoch [90/100    avg_loss:0.00, val_acc:0.98]
Epoch [91/100    avg_loss:0.01, val_acc:0.98]
Epoch [92/100    avg_loss:0.03, val_acc:0.96]
Epoch [93/100    avg_loss:0.01, val_acc:0.97]
Epoch [94/100    avg_loss:0.01, val_acc:0.97]
Epoch [95/100    avg_loss:0.01, val_acc:0.97]
Epoch [96/100    avg_loss:0.01, val_acc:0.97]
Epoch [97/100    avg_loss:0.01, val_acc:0.96]
Epoch [98/100    avg_loss:0.00, val_acc:0.97]
Epoch [99/100    avg_loss:0.00, val_acc:0.97]
Epoch [100/100    avg_loss:0.00, val_acc:0.98]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1279    2    0    0    1    0    0    0    3    0    0    0
     0    0    0]
 [   0    0    6  728    8    2    0    0    0    2    0    0    1    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    1    6    0    0    0    0    0    0    0  842   22    1    0
     3    0    0]
 [   0    0   18   12    0    0    0    0    0    0   30 2141    0    0
     5    0    4]
 [   0    0    0   12    0    0    0    0    0    0   10    0  504    0
     0    0    8]
 [   0    0    0    2    0    0    0    0    0    0    0    0    0  183
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    1    0    0    1   33    0    0    0    0    0    0    0
    74  238    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
96.9539

F1 scores:
[   nan 0.9756 0.9854 0.9687 0.9816 0.9908 0.974  1.     0.9977 0.9474
 0.9568 0.979  0.9637 0.9946 0.9632 0.8137 0.9091]

Kappa:
0.9653
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt3.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt3.npy)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [4/15]
RUN:3
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [4/15]
RUN:3
1024 samples selected for validation(over 10249)
Running an experiment with the Model_by_COA model
Train dataloader:30
Validation dataloader:29
----------Training parameters----------
spe_kerner:9
spa_kerner:5
dataset:IndianPines
model:Model_by_COA
comb:parallel
state:sta_dy
blocks:2
spa_head:16
spe_head:25
folder:../dataset/
cuda:0
run:15
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f2820c9b0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/100    avg_loss:4.18, val_acc:0.34]
Epoch [2/100    avg_loss:2.37, val_acc:0.30]
Epoch [3/100    avg_loss:1.68, val_acc:0.50]
Epoch [4/100    avg_loss:1.11, val_acc:0.69]
Epoch [5/100    avg_loss:0.91, val_acc:0.65]
Epoch [6/100    avg_loss:0.83, val_acc:0.69]
Epoch [7/100    avg_loss:0.64, val_acc:0.72]
Epoch [8/100    avg_loss:0.43, val_acc:0.77]
Epoch [9/100    avg_loss:0.44, val_acc:0.84]
Epoch [10/100    avg_loss:0.34, val_acc:0.88]
Epoch [11/100    avg_loss:0.27, val_acc:0.86]
Epoch [12/100    avg_loss:0.20, val_acc:0.85]
Epoch [13/100    avg_loss:0.17, val_acc:0.90]
Epoch [14/100    avg_loss:0.13, val_acc:0.82]
Epoch [15/100    avg_loss:0.14, val_acc:0.92]
Epoch [16/100    avg_loss:0.11, val_acc:0.91]
Epoch [17/100    avg_loss:0.24, val_acc:0.87]
Epoch [18/100    avg_loss:0.16, val_acc:0.93]
Epoch [19/100    avg_loss:0.10, val_acc:0.93]
Epoch [20/100    avg_loss:0.12, val_acc:0.92]
Epoch [21/100    avg_loss:0.07, val_acc:0.94]
Epoch [22/100    avg_loss:0.06, val_acc:0.93]
Epoch [23/100    avg_loss:0.07, val_acc:0.94]
Epoch [24/100    avg_loss:0.04, val_acc:0.96]
Epoch [25/100    avg_loss:0.03, val_acc:0.97]
Epoch [26/100    avg_loss:0.02, val_acc:0.97]
Epoch [27/100    avg_loss:0.02, val_acc:0.97]
Epoch [28/100    avg_loss:0.02, val_acc:0.97]
Epoch [29/100    avg_loss:0.02, val_acc:0.97]
Epoch [30/100    avg_loss:0.02, val_acc:0.96]
Epoch [31/100    avg_loss:0.02, val_acc:0.97]
Epoch [32/100    avg_loss:0.02, val_acc:0.96]
Epoch [33/100    avg_loss:0.01, val_acc:0.97]
Epoch [34/100    avg_loss:0.01, val_acc:0.97]
Epoch [35/100    avg_loss:0.02, val_acc:0.97]
Epoch [36/100    avg_loss:0.03, val_acc:0.96]
Epoch [37/100    avg_loss:0.02, val_acc:0.97]
Epoch [38/100    avg_loss:0.01, val_acc:0.96]
Epoch [39/100    avg_loss:0.01, val_acc:0.97]
Epoch [40/100    avg_loss:0.02, val_acc:0.97]
Epoch [41/100    avg_loss:0.01, val_acc:0.96]
Epoch [42/100    avg_loss:0.01, val_acc:0.97]
Epoch [43/100    avg_loss:0.01, val_acc:0.96]
Epoch [44/100    avg_loss:0.01, val_acc:0.97]
Epoch [45/100    avg_loss:0.01, val_acc:0.97]
Epoch [46/100    avg_loss:0.01, val_acc:0.97]
Epoch [47/100    avg_loss:0.01, val_acc:0.97]
Epoch [48/100    avg_loss:0.01, val_acc:0.97]
Epoch [49/100    avg_loss:0.01, val_acc:0.97]
Epoch [50/100    avg_loss:0.01, val_acc:0.97]
Epoch [51/100    avg_loss:0.02, val_acc:0.97]
Epoch [52/100    avg_loss:0.06, val_acc:0.95]
Epoch [53/100    avg_loss:0.04, val_acc:0.97]
Epoch [54/100    avg_loss:0.02, val_acc:0.97]
Epoch [55/100    avg_loss:0.02, val_acc:0.97]
Epoch [56/100    avg_loss:0.01, val_acc:0.98]
Epoch [57/100    avg_loss:0.01, val_acc:0.97]
Epoch [58/100    avg_loss:0.00, val_acc:0.98]
Epoch [59/100    avg_loss:0.02, val_acc:0.98]
Epoch [60/100    avg_loss:0.02, val_acc:0.97]
Epoch [61/100    avg_loss:0.00, val_acc:0.98]
Epoch [62/100    avg_loss:0.00, val_acc:0.98]
Epoch [63/100    avg_loss:0.01, val_acc:0.98]
Epoch [64/100    avg_loss:0.01, val_acc:0.97]
Epoch [65/100    avg_loss:0.00, val_acc:0.98]
Epoch [66/100    avg_loss:0.00, val_acc:0.98]
Epoch [67/100    avg_loss:0.00, val_acc:0.98]
Epoch [68/100    avg_loss:0.00, val_acc:0.98]
Epoch [69/100    avg_loss:0.00, val_acc:0.98]
Epoch [70/100    avg_loss:0.00, val_acc:0.97]
Epoch [71/100    avg_loss:0.01, val_acc:0.97]
Epoch [72/100    avg_loss:0.01, val_acc:0.98]
Epoch [73/100    avg_loss:0.01, val_acc:0.97]
Epoch [74/100    avg_loss:0.01, val_acc:0.96]
Epoch [75/100    avg_loss:0.01, val_acc:0.97]
Epoch [76/100    avg_loss:0.00, val_acc:0.98]
Epoch [77/100    avg_loss:0.00, val_acc:0.98]
Epoch [78/100    avg_loss:0.01, val_acc:0.96]
Epoch [79/100    avg_loss:0.01, val_acc:0.98]
Epoch [80/100    avg_loss:0.00, val_acc:0.98]
Epoch [81/100    avg_loss:0.00, val_acc:0.98]
Epoch [82/100    avg_loss:0.02, val_acc:0.96]
Epoch [83/100    avg_loss:0.01, val_acc:0.97]
Epoch [84/100    avg_loss:0.04, val_acc:0.96]
Epoch [85/100    avg_loss:0.01, val_acc:0.97]
Epoch [86/100    avg_loss:0.00, val_acc:0.97]
Epoch [87/100    avg_loss:0.01, val_acc:0.98]
Epoch [88/100    avg_loss:0.01, val_acc:0.97]
Epoch [89/100    avg_loss:0.02, val_acc:0.97]
Epoch [90/100    avg_loss:0.02, val_acc:0.97]
Epoch [91/100    avg_loss:0.02, val_acc:0.97]
Epoch [92/100    avg_loss:0.01, val_acc:0.97]
Epoch [93/100    avg_loss:0.00, val_acc:0.97]
Epoch [94/100    avg_loss:0.00, val_acc:0.98]
Epoch [95/100    avg_loss:0.01, val_acc:0.98]
Epoch [96/100    avg_loss:0.01, val_acc:0.97]
Epoch [97/100    avg_loss:0.00, val_acc:0.98]
Epoch [98/100    avg_loss:0.00, val_acc:0.98]
Epoch [99/100    avg_loss:0.00, val_acc:0.98]
Epoch [100/100    avg_loss:0.00, val_acc:0.98]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   30    0    0    0    3    0    0    8    0    0    0    0    0
     0    0    0]
 [   0    0 1265    0    0    0    6    0    0    0    5    9    0    0
     0    0    0]
 [   0    0   14  680    8   17    0    0    0    1    0    8   19    0
     0    0    0]
 [   0    0    0    3  204    0    0    0    0    0    0    0    6    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    1    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0   33    0    0    0    0    0    0    0  818   20    0    0
     3    1    0]
 [   0    0    8    0    0    0    0    0    0    0    0 2192   10    0
     0    0    0]
 [   0    0    9    8    6    0    0    0    0    0    2   19  472    0
     1    0   17]
 [   0    0    0    3    0    0    0    0    0    0    0    0    0  182
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1138    1    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
   108  239    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
95.9783

F1 scores:
[   nan 0.8451 0.9679 0.9425 0.9466 0.9707 0.9939 1.     0.9908 0.8824
 0.9624 0.9832 0.9025 0.9918 0.9503 0.8129 0.8778]

Kappa:
0.9541
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt4.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt4.npy)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [5/15]
RUN:4
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [5/15]
RUN:4
1024 samples selected for validation(over 10249)
Running an experiment with the Model_by_COA model
Train dataloader:30
Validation dataloader:29
----------Training parameters----------
spe_kerner:9
spa_kerner:5
dataset:IndianPines
model:Model_by_COA
comb:parallel
state:sta_dy
blocks:2
spa_head:16
spe_head:25
folder:../dataset/
cuda:0
run:15
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f2820c9b0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt5.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt5.npy)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [6/15]
RUN:5
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [6/15]
RUN:5
1024 samples selected for validation(over 10249)
Running an experiment with the Model_by_COA model
Train dataloader:30
Validation dataloader:29
----------Training parameters----------
spe_kerner:9
spa_kerner:5
dataset:IndianPines
model:Model_by_COA
comb:parallel
state:sta_dy
blocks:2
spa_head:16
spe_head:25
folder:../dataset/
cuda:0
run:15
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f2820c9b0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt6.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt6.npy)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [7/15]
RUN:6
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [7/15]
RUN:6
1024 samples selected for validation(over 10249)
Running an experiment with the Model_by_COA model
Train dataloader:29
Validation dataloader:30
----------Training parameters----------
spe_kerner:9
spa_kerner:5
dataset:IndianPines
model:Model_by_COA
comb:parallel
state:sta_dy
blocks:2
spa_head:16
spe_head:25
folder:../dataset/
cuda:0
run:15
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f2820c9b0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt7.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt7.npy)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [8/15]
RUN:7
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [8/15]
RUN:7
1024 samples selected for validation(over 10249)
Running an experiment with the Model_by_COA model
Train dataloader:30
Validation dataloader:29
----------Training parameters----------
spe_kerner:9
spa_kerner:5
dataset:IndianPines
model:Model_by_COA
comb:parallel
state:sta_dy
blocks:2
spa_head:16
spe_head:25
folder:../dataset/
cuda:0
run:15
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f2820c9b0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/100    avg_loss:3.94, val_acc:0.28]
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt8.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt8.npy)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [9/15]
RUN:8
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [9/15]
RUN:8
1024 samples selected for validation(over 10249)
Running an experiment with the Model_by_COA model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
spe_kerner:9
spa_kerner:5
dataset:IndianPines
model:Model_by_COA
comb:parallel
state:sta_dy
blocks:2
spa_head:16
spe_head:25
folder:../dataset/
cuda:0
run:15
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f2820c9b0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt9.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt9.npy)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [10/15]
RUN:9
Setting up a new session...
Visdom successfully connected to server
creating ./logs/logs-2022-06-28IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-06-28:17:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt0.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt0.npy)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the Model_by_COA model
Train dataloader:30
Validation dataloader:29
----------Training parameters----------
spe_kerner:9
spa_kerner:5
dataset:IndianPines
model:Model_by_COA
spa_head:16
spe_head:25
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3bb6594780>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/100    avg_loss:2.35, val_acc:0.56]
Epoch [2/100    avg_loss:1.23, val_acc:0.66]
Epoch [3/100    avg_loss:0.65, val_acc:0.85]
Epoch [4/100    avg_loss:0.44, val_acc:0.87]
Epoch [5/100    avg_loss:0.39, val_acc:0.88]
Epoch [6/100    avg_loss:0.35, val_acc:0.90]
Epoch [7/100    avg_loss:0.21, val_acc:0.94]
Epoch [8/100    avg_loss:0.18, val_acc:0.95]
Epoch [9/100    avg_loss:0.13, val_acc:0.98]
Epoch [10/100    avg_loss:0.13, val_acc:0.94]
Epoch [11/100    avg_loss:0.10, val_acc:0.94]
Epoch [12/100    avg_loss:0.08, val_acc:0.96]
Epoch [13/100    avg_loss:0.16, val_acc:0.94]
Epoch [14/100    avg_loss:0.07, val_acc:0.97]
Epoch [15/100    avg_loss:0.06, val_acc:0.97]
Epoch [16/100    avg_loss:0.04, val_acc:0.98]
Epoch [17/100    avg_loss:0.03, val_acc:0.97]
Epoch [18/100    avg_loss:0.05, val_acc:0.94]
Epoch [19/100    avg_loss:0.07, val_acc:0.98]
Epoch [20/100    avg_loss:0.03, val_acc:0.98]
Epoch [21/100    avg_loss:0.01, val_acc:0.98]
Epoch [22/100    avg_loss:0.03, val_acc:0.98]
Epoch [23/100    avg_loss:0.03, val_acc:0.98]
Epoch [24/100    avg_loss:0.01, val_acc:0.99]
Epoch [25/100    avg_loss:0.01, val_acc:0.99]
Epoch [26/100    avg_loss:0.01, val_acc:0.99]
Epoch [27/100    avg_loss:0.02, val_acc:0.98]
Epoch [28/100    avg_loss:0.02, val_acc:0.98]
Epoch [29/100    avg_loss:0.01, val_acc:0.98]
Epoch [30/100    avg_loss:0.01, val_acc:0.99]
Epoch [31/100    avg_loss:0.01, val_acc:0.99]
Epoch [32/100    avg_loss:0.01, val_acc:0.99]
creating ./logs/logs-2022-06-28IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-06-28:17:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt0.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt0.npy)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the Model_by_COA model
Train dataloader:30
Validation dataloader:29
----------Training parameters----------
spe_kerner:9
spa_kerner:5
dataset:IndianPines
model:Model_by_COA
spa_head:16
spe_head:25
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fde48a27780>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/100    avg_loss:2.31, val_acc:0.57]
Epoch [2/100    avg_loss:1.20, val_acc:0.66]
Epoch [3/100    avg_loss:0.93, val_acc:0.72]
Epoch [4/100    avg_loss:0.60, val_acc:0.84]
Epoch [5/100    avg_loss:0.42, val_acc:0.86]
Epoch [6/100    avg_loss:0.44, val_acc:0.87]
Epoch [7/100    avg_loss:0.32, val_acc:0.89]
Epoch [8/100    avg_loss:0.23, val_acc:0.93]
Epoch [9/100    avg_loss:0.20, val_acc:0.92]
Epoch [10/100    avg_loss:0.15, val_acc:0.94]
Epoch [11/100    avg_loss:0.11, val_acc:0.96]
Epoch [12/100    avg_loss:0.05, val_acc:0.96]
Epoch [13/100    avg_loss:0.04, val_acc:0.97]
Epoch [14/100    avg_loss:0.04, val_acc:0.97]
Epoch [15/100    avg_loss:0.04, val_acc:0.97]
Epoch [16/100    avg_loss:0.03, val_acc:0.97]
Epoch [17/100    avg_loss:0.03, val_acc:0.98]
Epoch [18/100    avg_loss:0.02, val_acc:0.98]
Epoch [19/100    avg_loss:0.02, val_acc:0.97]
Epoch [20/100    avg_loss:0.06, val_acc:0.95]
Epoch [21/100    avg_loss:0.06, val_acc:0.92]
Epoch [22/100    avg_loss:0.09, val_acc:0.96]
Epoch [23/100    avg_loss:0.04, val_acc:0.96]
Epoch [24/100    avg_loss:0.03, val_acc:0.96]
Epoch [25/100    avg_loss:0.03, val_acc:0.98]
Epoch [26/100    avg_loss:0.05, val_acc:0.96]
Epoch [27/100    avg_loss:0.03, val_acc:0.97]
Epoch [28/100    avg_loss:0.03, val_acc:0.97]
Epoch [29/100    avg_loss:0.01, val_acc:0.97]
Epoch [30/100    avg_loss:0.01, val_acc:0.97]
Epoch [31/100    avg_loss:0.01, val_acc:0.98]
Epoch [32/100    avg_loss:0.01, val_acc:0.98]
Epoch [33/100    avg_loss:0.01, val_acc:0.98]
Epoch [34/100    avg_loss:0.01, val_acc:0.97]
Epoch [35/100    avg_loss:0.01, val_acc:0.98]
Epoch [36/100    avg_loss:0.01, val_acc:0.97]
Epoch [37/100    avg_loss:0.01, val_acc:0.97]
Epoch [38/100    avg_loss:0.00, val_acc:0.97]
Epoch [39/100    avg_loss:0.01, val_acc:0.97]
Epoch [40/100    avg_loss:0.01, val_acc:0.98]
Epoch [41/100    avg_loss:0.00, val_acc:0.98]
Epoch [42/100    avg_loss:0.00, val_acc:0.98]
Epoch [43/100    avg_loss:0.01, val_acc:0.97]
Epoch [44/100    avg_loss:0.02, val_acc:0.97]
Epoch [45/100    avg_loss:0.01, val_acc:0.98]
Epoch [46/100    avg_loss:0.00, val_acc:0.98]
Epoch [47/100    avg_loss:0.00, val_acc:0.98]
Epoch [48/100    avg_loss:0.00, val_acc:0.98]
Epoch [49/100    avg_loss:0.00, val_acc:0.98]
Epoch [50/100    avg_loss:0.00, val_acc:0.98]
Epoch [51/100    avg_loss:0.00, val_acc:0.98]
Epoch [52/100    avg_loss:0.00, val_acc:0.98]
Epoch [53/100    avg_loss:0.00, val_acc:0.98]
Epoch [54/100    avg_loss:0.00, val_acc:0.98]
Epoch [55/100    avg_loss:0.00, val_acc:0.98]
Epoch [56/100    avg_loss:0.00, val_acc:0.98]
Epoch [57/100    avg_loss:0.00, val_acc:0.98]
Epoch [58/100    avg_loss:0.00, val_acc:0.98]
Epoch [59/100    avg_loss:0.00, val_acc:0.98]
Epoch [60/100    avg_loss:0.00, val_acc:0.98]
Epoch [61/100    avg_loss:0.00, val_acc:0.98]
Epoch [62/100    avg_loss:0.00, val_acc:0.98]
Epoch [63/100    avg_loss:0.00, val_acc:0.98]
Epoch [64/100    avg_loss:0.00, val_acc:0.98]
Epoch [65/100    avg_loss:0.00, val_acc:0.98]
Epoch [66/100    avg_loss:0.00, val_acc:0.98]
Epoch [67/100    avg_loss:0.00, val_acc:0.98]
Epoch [68/100    avg_loss:0.00, val_acc:0.98]
Epoch [69/100    avg_loss:0.00, val_acc:0.98]
Epoch [70/100    avg_loss:0.00, val_acc:0.98]
Epoch [71/100    avg_loss:0.00, val_acc:0.98]
Epoch [72/100    avg_loss:0.00, val_acc:0.98]
Epoch [73/100    avg_loss:0.00, val_acc:0.98]
Epoch [74/100    avg_loss:0.00, val_acc:0.98]
Epoch [75/100    avg_loss:0.01, val_acc:0.97]
Epoch [76/100    avg_loss:0.02, val_acc:0.98]
Epoch [77/100    avg_loss:0.01, val_acc:0.97]
Epoch [78/100    avg_loss:0.01, val_acc:0.98]
Epoch [79/100    avg_loss:0.01, val_acc:0.98]
Epoch [80/100    avg_loss:0.04, val_acc:0.98]
Epoch [81/100    avg_loss:0.01, val_acc:0.98]
Epoch [82/100    avg_loss:0.00, val_acc:0.98]
Epoch [83/100    avg_loss:0.00, val_acc:0.98]
Epoch [84/100    avg_loss:0.00, val_acc:0.97]
Epoch [85/100    avg_loss:0.01, val_acc:0.97]
Epoch [86/100    avg_loss:0.00, val_acc:0.97]
Epoch [87/100    avg_loss:0.00, val_acc:0.98]
Epoch [88/100    avg_loss:0.00, val_acc:0.98]
Epoch [89/100    avg_loss:0.00, val_acc:0.98]
Epoch [90/100    avg_loss:0.00, val_acc:0.98]
Epoch [91/100    avg_loss:0.00, val_acc:0.98]
Epoch [92/100    avg_loss:0.01, val_acc:0.98]
Epoch [93/100    avg_loss:0.00, val_acc:0.98]
Epoch [94/100    avg_loss:0.00, val_acc:0.98]
Epoch [95/100    avg_loss:0.00, val_acc:0.98]
Epoch [96/100    avg_loss:0.00, val_acc:0.98]
Epoch [97/100    avg_loss:0.00, val_acc:0.97]
Epoch [98/100    avg_loss:0.03, val_acc:0.97]
Epoch [99/100    avg_loss:0.03, val_acc:0.97]
Epoch [100/100    avg_loss:0.04, val_acc:0.97]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   32    4    0    0    0    0    0    2    0    3    0    0    0
     0    0    0]
 [   0    0 1248    7    0    0    1    0    0    0    4   22    3    0
     0    0    0]
 [   0    0    5  625    0    0    0    0    0    0    0   10  106    0
     0    1    0]
 [   0    0    0    2  201    0    0    0    0    0    0    0   10    0
     0    0    0]
 [   0    1    0    0    0  416    0    0    0    0    5    0    6    1
     6    0    0]
 [   0    0    0    0    0    0  646    0    0    1    1    8    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    1    1    0    0   12    0    0    0    0
     0    0    0]
 [   0    0   16    0    0    0    0    0    0    0  825   20    4    0
    10    0    0]
 [   0    0    0    3    0    1    0    0    0    0    2 2200    4    0
     0    0    0]
 [   0    0    0    9    0    0    0    0    0    0    3    0  521    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1138    1    0]
 [   0    0    0    0    0    0    1    0    0    0    0    4    0    0
   105  237    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0   18    0
     0    0   66]]

Accuracy:
95.4688

F1 scores:
[   nan 0.8649 0.9758 0.8948 0.971  0.9754 0.9893 1.     0.9977 0.7742
 0.9604 0.9835 0.864  0.9973 0.9487 0.8089 0.8742]

Kappa:
0.9482
creating ./logs/logs-2022-06-28IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-06-28:17:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt0.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt0.npy)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the Model_by_COA model
Train dataloader:30
Validation dataloader:29
----------Training parameters----------
spe_kerner:9
spa_kerner:5
dataset:IndianPines
model:Model_by_COA
spa_head:16
spe_head:25
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f047bf18748>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/100    avg_loss:2.17, val_acc:0.55]
Epoch [2/100    avg_loss:1.18, val_acc:0.69]
Epoch [3/100    avg_loss:0.56, val_acc:0.82]
Epoch [4/100    avg_loss:0.41, val_acc:0.88]
Epoch [5/100    avg_loss:0.25, val_acc:0.91]
Epoch [6/100    avg_loss:0.25, val_acc:0.87]
Epoch [7/100    avg_loss:0.19, val_acc:0.94]
Epoch [8/100    avg_loss:0.15, val_acc:0.93]
Epoch [9/100    avg_loss:0.11, val_acc:0.94]
Epoch [10/100    avg_loss:0.07, val_acc:0.96]
Epoch [11/100    avg_loss:0.10, val_acc:0.95]
Epoch [12/100    avg_loss:0.05, val_acc:0.95]
Epoch [13/100    avg_loss:0.07, val_acc:0.96]
Epoch [14/100    avg_loss:0.07, val_acc:0.94]
Epoch [15/100    avg_loss:0.06, val_acc:0.95]
Epoch [16/100    avg_loss:0.04, val_acc:0.96]
Epoch [17/100    avg_loss:0.03, val_acc:0.97]
Epoch [18/100    avg_loss:0.03, val_acc:0.97]
Epoch [19/100    avg_loss:0.02, val_acc:0.97]
Epoch [20/100    avg_loss:0.01, val_acc:0.96]
Epoch [21/100    avg_loss:0.03, val_acc:0.95]
Epoch [22/100    avg_loss:0.04, val_acc:0.96]
Epoch [23/100    avg_loss:0.04, val_acc:0.98]
Epoch [24/100    avg_loss:0.02, val_acc:0.97]
Epoch [25/100    avg_loss:0.01, val_acc:0.97]
Epoch [26/100    avg_loss:0.02, val_acc:0.96]
Epoch [27/100    avg_loss:0.01, val_acc:0.98]
creating ./logs/logs-2022-06-28IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-06-28:17:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt0.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt0.npy)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the Model_by_COA model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
spe_kerner:9
spa_kerner:5
dataset:IndianPines
model:Model_by_COA
spa_head:16
spe_head:25
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f654ff97780>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/100    avg_loss:2.21, val_acc:0.46]
Epoch [2/100    avg_loss:1.10, val_acc:0.69]
Epoch [3/100    avg_loss:0.69, val_acc:0.73]
Epoch [4/100    avg_loss:0.45, val_acc:0.80]
Epoch [5/100    avg_loss:0.41, val_acc:0.87]
Epoch [6/100    avg_loss:0.27, val_acc:0.92]
Epoch [7/100    avg_loss:0.15, val_acc:0.91]
Epoch [8/100    avg_loss:0.20, val_acc:0.89]
Epoch [9/100    avg_loss:0.16, val_acc:0.91]
Epoch [10/100    avg_loss:0.10, val_acc:0.94]
Epoch [11/100    avg_loss:0.08, val_acc:0.95]
Epoch [12/100    avg_loss:0.05, val_acc:0.95]
Epoch [13/100    avg_loss:0.07, val_acc:0.95]
Epoch [14/100    avg_loss:0.04, val_acc:0.96]
Epoch [15/100    avg_loss:0.08, val_acc:0.83]
Epoch [16/100    avg_loss:0.24, val_acc:0.88]
Epoch [17/100    avg_loss:0.16, val_acc:0.91]
Epoch [18/100    avg_loss:0.07, val_acc:0.94]
Epoch [19/100    avg_loss:0.05, val_acc:0.94]
Epoch [20/100    avg_loss:0.07, val_acc:0.96]
Epoch [21/100    avg_loss:0.06, val_acc:0.96]
Epoch [22/100    avg_loss:0.04, val_acc:0.91]
Epoch [23/100    avg_loss:0.06, val_acc:0.94]
Epoch [24/100    avg_loss:0.04, val_acc:0.94]
Epoch [25/100    avg_loss:0.01, val_acc:0.96]
Epoch [26/100    avg_loss:0.02, val_acc:0.96]
Epoch [27/100    avg_loss:0.04, val_acc:0.96]
Epoch [28/100    avg_loss:0.01, val_acc:0.96]
Epoch [29/100    avg_loss:0.01, val_acc:0.97]
Epoch [30/100    avg_loss:0.02, val_acc:0.97]
Epoch [31/100    avg_loss:0.01, val_acc:0.97]
Epoch [32/100    avg_loss:0.00, val_acc:0.97]
Epoch [33/100    avg_loss:0.01, val_acc:0.97]
Epoch [34/100    avg_loss:0.00, val_acc:0.97]
Epoch [35/100    avg_loss:0.01, val_acc:0.97]
Epoch [36/100    avg_loss:0.01, val_acc:0.97]
Epoch [37/100    avg_loss:0.01, val_acc:0.97]
Epoch [38/100    avg_loss:0.02, val_acc:0.97]
Epoch [39/100    avg_loss:0.01, val_acc:0.96]
Epoch [40/100    avg_loss:0.01, val_acc:0.97]
Epoch [41/100    avg_loss:0.02, val_acc:0.97]
Epoch [42/100    avg_loss:0.01, val_acc:0.97]
Epoch [43/100    avg_loss:0.01, val_acc:0.97]
Epoch [44/100    avg_loss:0.01, val_acc:0.98]
Epoch [45/100    avg_loss:0.00, val_acc:0.98]
Epoch [46/100    avg_loss:0.00, val_acc:0.98]
Epoch [47/100    avg_loss:0.00, val_acc:0.98]
Epoch [48/100    avg_loss:0.04, val_acc:0.97]
Epoch [49/100    avg_loss:0.01, val_acc:0.98]
Epoch [50/100    avg_loss:0.01, val_acc:0.97]
Epoch [51/100    avg_loss:0.00, val_acc:0.98]
Epoch [52/100    avg_loss:0.00, val_acc:0.98]
Epoch [53/100    avg_loss:0.01, val_acc:0.97]
Epoch [54/100    avg_loss:0.01, val_acc:0.97]
Epoch [55/100    avg_loss:0.00, val_acc:0.97]
Epoch [56/100    avg_loss:0.00, val_acc:0.98]
Epoch [57/100    avg_loss:0.00, val_acc:0.97]
Epoch [58/100    avg_loss:0.00, val_acc:0.97]
Epoch [59/100    avg_loss:0.00, val_acc:0.97]
Epoch [60/100    avg_loss:0.00, val_acc:0.98]
Epoch [61/100    avg_loss:0.00, val_acc:0.98]
Epoch [62/100    avg_loss:0.00, val_acc:0.98]
Epoch [63/100    avg_loss:0.00, val_acc:0.98]
Epoch [64/100    avg_loss:0.00, val_acc:0.98]
Epoch [65/100    avg_loss:0.00, val_acc:0.98]
Epoch [66/100    avg_loss:0.00, val_acc:0.98]
Epoch [67/100    avg_loss:0.00, val_acc:0.98]
Epoch [68/100    avg_loss:0.00, val_acc:0.98]
Epoch [69/100    avg_loss:0.00, val_acc:0.98]
Epoch [70/100    avg_loss:0.00, val_acc:0.98]
Epoch [71/100    avg_loss:0.00, val_acc:0.98]
Epoch [72/100    avg_loss:0.00, val_acc:0.98]
Epoch [73/100    avg_loss:0.00, val_acc:0.98]
Epoch [74/100    avg_loss:0.00, val_acc:0.98]
Epoch [75/100    avg_loss:0.00, val_acc:0.98]
Epoch [76/100    avg_loss:0.00, val_acc:0.98]
Epoch [77/100    avg_loss:0.01, val_acc:0.98]
Epoch [78/100    avg_loss:0.02, val_acc:0.96]
Epoch [79/100    avg_loss:0.02, val_acc:0.97]
Epoch [80/100    avg_loss:0.00, val_acc:0.97]
Epoch [81/100    avg_loss:0.00, val_acc:0.97]
Epoch [82/100    avg_loss:0.01, val_acc:0.98]
Epoch [83/100    avg_loss:0.00, val_acc:0.97]
Epoch [84/100    avg_loss:0.00, val_acc:0.97]
Epoch [85/100    avg_loss:0.00, val_acc:0.97]
Epoch [86/100    avg_loss:0.00, val_acc:0.98]
Epoch [87/100    avg_loss:0.00, val_acc:0.97]
Epoch [88/100    avg_loss:0.00, val_acc:0.97]
Epoch [89/100    avg_loss:0.00, val_acc:0.97]
Epoch [90/100    avg_loss:0.00, val_acc:0.98]
Epoch [91/100    avg_loss:0.00, val_acc:0.98]
Epoch [92/100    avg_loss:0.00, val_acc:0.98]
Epoch [93/100    avg_loss:0.00, val_acc:0.98]
Epoch [94/100    avg_loss:0.00, val_acc:0.98]
Epoch [95/100    avg_loss:0.00, val_acc:0.98]
Epoch [96/100    avg_loss:0.00, val_acc:0.98]
Epoch [97/100    avg_loss:0.01, val_acc:0.98]
Epoch [98/100    avg_loss:0.00, val_acc:0.98]
Epoch [99/100    avg_loss:0.00, val_acc:0.98]
Epoch [100/100    avg_loss:0.00, val_acc:0.98]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   28    4    0    0    0    0    0    9    0    0    0    0    0
     0    0    0]
 [   0    0 1269    1    0    0    3    0    0    0    2   10    0    0
     0    0    0]
 [   0    0    1  737    0    0    0    0    0    0    0    2    7    0
     0    0    0]
 [   0    0    0    0  207    0    0    0    0    0    0    0    6    0
     0    0    0]
 [   0    0    0    5    0  420    0    0    0    0    3    1    0    0
     6    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    7    0    0   11    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  856    1   13    0
     1    0    0]
 [   0    0   14    0    0    0    0    0    0    0    2 2194    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0    1    0  532    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0   58    0    0    0    0    0    0   12
    45  232    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    7    0
     0    0   77]]

Accuracy:
97.5393

F1 scores:
[   nan 0.8116 0.9845 0.9893 0.9857 0.9825 0.95   1.     0.9896 0.7586
 0.9845 0.993  0.9682 0.9686 0.9777 0.8014 0.9565]

Kappa:
0.9719
dataset:IndianPines
['81.16+-0.0' '98.45+-0.0' '98.93+-0.0' '98.57+-0.0' '98.25+-0.0'
 '95.0+-0.0' '100.0+-0.0' '98.96+-0.0' '75.86+-0.0' '98.45+-0.0'
 '99.3+-0.0' '96.82+-0.0' '96.86+-0.0' '97.77+-0.0' '80.14+-0.0'
 '95.65+-0.0']
OA_list [[97.53929539]]
OA±std 97.54 ±0.00
creating ./logs/logs-2022-06-28IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-06-28:19:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Running an experiment with the Model_by_COA model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the Model_by_COA model
Train dataloader:29
Validation dataloader:30
----------Training parameters----------
spe_kerner:9
spa_kerner:5
dataset:IndianPines
model:Model_by_COA
spa_head:16
spe_head:25
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa2d62ad710>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/100    avg_loss:2.23, val_acc:0.57]
Epoch [2/100    avg_loss:1.21, val_acc:0.59]
Epoch [3/100    avg_loss:0.76, val_acc:0.74]
Epoch [4/100    avg_loss:0.57, val_acc:0.84]
Epoch [5/100    avg_loss:0.43, val_acc:0.85]
Epoch [6/100    avg_loss:0.37, val_acc:0.86]
Epoch [7/100    avg_loss:0.24, val_acc:0.92]
Epoch [8/100    avg_loss:0.20, val_acc:0.88]
Epoch [9/100    avg_loss:0.12, val_acc:0.94]
Epoch [10/100    avg_loss:0.17, val_acc:0.91]
Epoch [11/100    avg_loss:0.18, val_acc:0.90]
Epoch [12/100    avg_loss:0.17, val_acc:0.92]
Epoch [13/100    avg_loss:0.09, val_acc:0.93]
Epoch [14/100    avg_loss:0.06, val_acc:0.95]
Epoch [15/100    avg_loss:0.03, val_acc:0.96]
Epoch [16/100    avg_loss:0.03, val_acc:0.95]
Epoch [17/100    avg_loss:0.04, val_acc:0.95]
Epoch [18/100    avg_loss:0.08, val_acc:0.95]
Epoch [19/100    avg_loss:0.03, val_acc:0.95]
Epoch [20/100    avg_loss:0.03, val_acc:0.96]
Epoch [21/100    avg_loss:0.02, val_acc:0.97]
Epoch [22/100    avg_loss:0.02, val_acc:0.96]
Epoch [23/100    avg_loss:0.03, val_acc:0.95]
Epoch [24/100    avg_loss:0.03, val_acc:0.94]
Epoch [25/100    avg_loss:0.07, val_acc:0.94]
Epoch [26/100    avg_loss:0.04, val_acc:0.95]
Epoch [27/100    avg_loss:0.02, val_acc:0.96]
Epoch [28/100    avg_loss:0.02, val_acc:0.97]
Epoch [29/100    avg_loss:0.02, val_acc:0.97]
Epoch [30/100    avg_loss:0.01, val_acc:0.97]
Epoch [31/100    avg_loss:0.01, val_acc:0.97]
Epoch [32/100    avg_loss:0.01, val_acc:0.98]
Epoch [33/100    avg_loss:0.01, val_acc:0.97]
Epoch [34/100    avg_loss:0.01, val_acc:0.97]
Epoch [35/100    avg_loss:0.00, val_acc:0.97]
Epoch [36/100    avg_loss:0.01, val_acc:0.97]
Epoch [37/100    avg_loss:0.00, val_acc:0.97]
Epoch [38/100    avg_loss:0.00, val_acc:0.97]
Epoch [39/100    avg_loss:0.00, val_acc:0.97]
Epoch [40/100    avg_loss:0.00, val_acc:0.97]
Epoch [41/100    avg_loss:0.00, val_acc:0.97]
Epoch [42/100    avg_loss:0.00, val_acc:0.97]
Epoch [43/100    avg_loss:0.00, val_acc:0.98]
Epoch [44/100    avg_loss:0.00, val_acc:0.97]
Epoch [45/100    avg_loss:0.00, val_acc:0.97]
Epoch [46/100    avg_loss:0.00, val_acc:0.98]
Epoch [47/100    avg_loss:0.00, val_acc:0.97]
Epoch [48/100    avg_loss:0.00, val_acc:0.97]
Epoch [49/100    avg_loss:0.00, val_acc:0.97]
Epoch [50/100    avg_loss:0.01, val_acc:0.96]
Epoch [51/100    avg_loss:0.01, val_acc:0.96]
Epoch [52/100    avg_loss:0.01, val_acc:0.97]
Epoch [53/100    avg_loss:0.00, val_acc:0.98]
Epoch [54/100    avg_loss:0.00, val_acc:0.97]
Epoch [55/100    avg_loss:0.00, val_acc:0.97]
Epoch [56/100    avg_loss:0.00, val_acc:0.98]
Epoch [57/100    avg_loss:0.00, val_acc:0.97]
Epoch [58/100    avg_loss:0.00, val_acc:0.97]
Epoch [59/100    avg_loss:0.00, val_acc:0.97]
Epoch [60/100    avg_loss:0.00, val_acc:0.97]
Epoch [61/100    avg_loss:0.00, val_acc:0.97]
Epoch [62/100    avg_loss:0.00, val_acc:0.97]
Epoch [63/100    avg_loss:0.00, val_acc:0.98]
Epoch [64/100    avg_loss:0.00, val_acc:0.97]
Epoch [65/100    avg_loss:0.01, val_acc:0.97]
Epoch [66/100    avg_loss:0.00, val_acc:0.97]
Epoch [67/100    avg_loss:0.00, val_acc:0.97]
Epoch [68/100    avg_loss:0.00, val_acc:0.97]
Epoch [69/100    avg_loss:0.00, val_acc:0.97]
Epoch [70/100    avg_loss:0.00, val_acc:0.98]
Epoch [71/100    avg_loss:0.00, val_acc:0.98]
Epoch [72/100    avg_loss:0.00, val_acc:0.98]
Epoch [73/100    avg_loss:0.00, val_acc:0.97]
Epoch [74/100    avg_loss:0.00, val_acc:0.98]
Epoch [75/100    avg_loss:0.00, val_acc:0.98]
Epoch [76/100    avg_loss:0.00, val_acc:0.97]
Epoch [77/100    avg_loss:0.00, val_acc:0.97]
Epoch [78/100    avg_loss:0.00, val_acc:0.98]
Epoch [79/100    avg_loss:0.00, val_acc:0.97]
Epoch [80/100    avg_loss:0.00, val_acc:0.97]
Epoch [81/100    avg_loss:0.00, val_acc:0.98]
Epoch [82/100    avg_loss:0.00, val_acc:0.98]
Epoch [83/100    avg_loss:0.00, val_acc:0.98]
Epoch [84/100    avg_loss:0.00, val_acc:0.98]
Epoch [85/100    avg_loss:0.00, val_acc:0.97]
Epoch [86/100    avg_loss:0.00, val_acc:0.98]
Epoch [87/100    avg_loss:0.00, val_acc:0.98]
Epoch [88/100    avg_loss:0.00, val_acc:0.98]
Epoch [89/100    avg_loss:0.00, val_acc:0.98]
Epoch [90/100    avg_loss:0.00, val_acc:0.98]
Epoch [91/100    avg_loss:0.00, val_acc:0.98]
Epoch [92/100    avg_loss:0.00, val_acc:0.98]
Epoch [93/100    avg_loss:0.00, val_acc:0.98]
Epoch [94/100    avg_loss:0.00, val_acc:0.98]
Epoch [95/100    avg_loss:0.00, val_acc:0.98]
Epoch [96/100    avg_loss:0.00, val_acc:0.98]
Epoch [97/100    avg_loss:0.00, val_acc:0.98]
Epoch [98/100    avg_loss:0.00, val_acc:0.98]
Epoch [99/100    avg_loss:0.00, val_acc:0.98]
Epoch [100/100    avg_loss:0.00, val_acc:0.98]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   32    9    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1245    2    0    0    1    0    0    0   26    6    5    0
     0    0    0]
 [   0    0    5  738    2    0    0    0    0    0    1    1    0    0
     0    0    0]
 [   0    0    0    0  199    0    0    0    0    1    0    0   13    0
     0    0    0]
 [   0    0    0    1    0  428    0    0    0    0    0    0    0    0
     6    0    0]
 [   0    0    1    0    0    0  656    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    3    0    0    0    0  863    2    0    0
     0    3    0]
 [   0    0   17    0    0    0    0    0    0    0   17 2176    0    0
     0    0    0]
 [   0    0    0    5    4    0    0    0    0    1   23    6  489    0
     0    5    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0    1    0    0    0
  1132    5    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    1    0
    82  248    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.9648

F1 scores:
[   nan 0.8767 0.97   0.9866 0.9522 0.9885 0.9865 1.     1.     0.8571
 0.9557 0.9889 0.9386 1.     0.9597 0.8158 0.9941]

Kappa:
0.9654
dataset:IndianPines
['87.67+-0.0' '97.0+-0.0' '98.66+-0.0' '95.22+-0.0' '98.85+-0.0'
 '98.65+-0.0' '100.0+-0.0' '100.0+-0.0' '85.71+-0.0' '95.57+-0.0'
 '98.89+-0.0' '93.86+-0.0' '100.0+-0.0' '95.97+-0.0' '81.58+-0.0'
 '99.41+-0.0']
OA_list [[96.96476965]]
OA±std 96.96 ±0.00
creating ./logs/logs-2022-06-28IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-06-28:19:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Running an experiment with the Model_by_COA model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the Model_by_COA model
Train dataloader:29
Validation dataloader:30
----------Training parameters----------
spe_kerner:9
spa_kerner:5
dataset:IndianPines
model:Model_by_COA
spa_head:16
spe_head:25
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff16f66f588>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/100    avg_loss:2.57, val_acc:0.55]
Epoch [2/100    avg_loss:1.33, val_acc:0.55]
Epoch [3/100    avg_loss:1.04, val_acc:0.75]
Epoch [4/100    avg_loss:0.71, val_acc:0.82]
Epoch [5/100    avg_loss:0.44, val_acc:0.91]
Epoch [6/100    avg_loss:0.28, val_acc:0.92]
Epoch [7/100    avg_loss:0.23, val_acc:0.93]
Epoch [8/100    avg_loss:0.17, val_acc:0.92]
Epoch [9/100    avg_loss:0.20, val_acc:0.93]
Epoch [10/100    avg_loss:0.17, val_acc:0.93]
Epoch [11/100    avg_loss:0.14, val_acc:0.95]
Epoch [12/100    avg_loss:0.14, val_acc:0.94]
Epoch [13/100    avg_loss:0.11, val_acc:0.95]
Epoch [14/100    avg_loss:0.09, val_acc:0.97]
Epoch [15/100    avg_loss:0.05, val_acc:0.97]
Epoch [16/100    avg_loss:0.06, val_acc:0.94]
Epoch [17/100    avg_loss:0.09, val_acc:0.94]
Epoch [18/100    avg_loss:0.05, val_acc:0.97]
Epoch [19/100    avg_loss:0.03, val_acc:0.97]
Epoch [20/100    avg_loss:0.03, val_acc:0.96]
Epoch [21/100    avg_loss:0.04, val_acc:0.96]
Epoch [22/100    avg_loss:0.02, val_acc:0.97]
Epoch [23/100    avg_loss:0.01, val_acc:0.98]
Epoch [24/100    avg_loss:0.01, val_acc:0.98]
Epoch [25/100    avg_loss:0.01, val_acc:0.98]
Epoch [26/100    avg_loss:0.01, val_acc:0.98]
Epoch [27/100    avg_loss:0.01, val_acc:0.97]
Epoch [28/100    avg_loss:0.01, val_acc:0.98]
Epoch [29/100    avg_loss:0.01, val_acc:0.98]
Epoch [30/100    avg_loss:0.01, val_acc:0.98]
Epoch [31/100    avg_loss:0.01, val_acc:0.98]
Epoch [32/100    avg_loss:0.01, val_acc:0.98]
Epoch [33/100    avg_loss:0.01, val_acc:0.98]
Epoch [34/100    avg_loss:0.01, val_acc:0.97]
Epoch [35/100    avg_loss:0.01, val_acc:0.97]
Epoch [36/100    avg_loss:0.01, val_acc:0.98]
Epoch [37/100    avg_loss:0.00, val_acc:0.98]
Epoch [38/100    avg_loss:0.00, val_acc:0.98]
Epoch [39/100    avg_loss:0.01, val_acc:0.98]
Epoch [40/100    avg_loss:0.01, val_acc:0.97]
Epoch [41/100    avg_loss:0.01, val_acc:0.98]
Epoch [42/100    avg_loss:0.00, val_acc:0.98]
Epoch [43/100    avg_loss:0.00, val_acc:0.98]
Epoch [44/100    avg_loss:0.00, val_acc:0.98]
Epoch [45/100    avg_loss:0.00, val_acc:0.98]
Epoch [46/100    avg_loss:0.00, val_acc:0.98]
Epoch [47/100    avg_loss:0.01, val_acc:0.95]
Epoch [48/100    avg_loss:0.02, val_acc:0.97]
Epoch [49/100    avg_loss:0.01, val_acc:0.98]
Epoch [50/100    avg_loss:0.00, val_acc:0.98]
Epoch [51/100    avg_loss:0.00, val_acc:0.98]
Epoch [52/100    avg_loss:0.00, val_acc:0.98]
Epoch [53/100    avg_loss:0.00, val_acc:0.98]
Epoch [54/100    avg_loss:0.01, val_acc:0.97]
Epoch [55/100    avg_loss:0.00, val_acc:0.98]
Epoch [56/100    avg_loss:0.00, val_acc:0.98]
Epoch [57/100    avg_loss:0.00, val_acc:0.98]
Epoch [58/100    avg_loss:0.00, val_acc:0.98]
Epoch [59/100    avg_loss:0.00, val_acc:0.98]
Epoch [60/100    avg_loss:0.00, val_acc:0.98]
Epoch [61/100    avg_loss:0.00, val_acc:0.98]
Epoch [62/100    avg_loss:0.00, val_acc:0.98]
Epoch [63/100    avg_loss:0.00, val_acc:0.98]
Epoch [64/100    avg_loss:0.00, val_acc:0.98]
Epoch [65/100    avg_loss:0.00, val_acc:0.98]
Epoch [66/100    avg_loss:0.00, val_acc:0.98]
Epoch [67/100    avg_loss:0.00, val_acc:0.98]
Epoch [68/100    avg_loss:0.00, val_acc:0.98]
Epoch [69/100    avg_loss:0.00, val_acc:0.98]
Epoch [70/100    avg_loss:0.00, val_acc:0.98]
Epoch [71/100    avg_loss:0.00, val_acc:0.98]
Epoch [72/100    avg_loss:0.00, val_acc:0.98]
Epoch [73/100    avg_loss:0.00, val_acc:0.98]
Epoch [74/100    avg_loss:0.00, val_acc:0.98]
Epoch [75/100    avg_loss:0.00, val_acc:0.98]
Epoch [76/100    avg_loss:0.00, val_acc:0.98]
Epoch [77/100    avg_loss:0.00, val_acc:0.98]
Epoch [78/100    avg_loss:0.00, val_acc:0.98]
Epoch [79/100    avg_loss:0.00, val_acc:0.98]
Epoch [80/100    avg_loss:0.00, val_acc:0.98]
Epoch [81/100    avg_loss:0.00, val_acc:0.98]
Epoch [82/100    avg_loss:0.00, val_acc:0.98]
Epoch [83/100    avg_loss:0.00, val_acc:0.98]
Epoch [84/100    avg_loss:0.00, val_acc:0.98]
Epoch [85/100    avg_loss:0.00, val_acc:0.98]
Epoch [86/100    avg_loss:0.00, val_acc:0.98]
Epoch [87/100    avg_loss:0.00, val_acc:0.98]
Epoch [88/100    avg_loss:0.02, val_acc:0.97]
Epoch [89/100    avg_loss:0.01, val_acc:0.98]
Epoch [90/100    avg_loss:0.01, val_acc:0.98]
Epoch [91/100    avg_loss:0.00, val_acc:0.97]
Epoch [92/100    avg_loss:0.00, val_acc:0.98]
Epoch [93/100    avg_loss:0.00, val_acc:0.98]
Epoch [94/100    avg_loss:0.00, val_acc:0.98]
Epoch [95/100    avg_loss:0.00, val_acc:0.98]
Epoch [96/100    avg_loss:0.00, val_acc:0.98]
Epoch [97/100    avg_loss:0.00, val_acc:0.98]
Epoch [98/100    avg_loss:0.00, val_acc:0.99]
Epoch [99/100    avg_loss:0.00, val_acc:0.99]
Epoch [100/100    avg_loss:0.00, val_acc:0.98]
creating ./logs/logs-2022-06-28IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-06-28:19:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Running an experiment with the Model_by_COA model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the Model_by_COA model
Train dataloader:29
Validation dataloader:30
----------Training parameters----------
spe_kerner:9
spa_kerner:5
dataset:IndianPines
model:Model_by_COA
spa_head:16
spe_head:25
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f70a43ee6d8>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/100    avg_loss:2.23, val_acc:0.54]
Epoch [2/100    avg_loss:1.09, val_acc:0.66]
Epoch [3/100    avg_loss:0.69, val_acc:0.78]
Epoch [4/100    avg_loss:0.46, val_acc:0.84]
Epoch [5/100    avg_loss:0.26, val_acc:0.86]
Epoch [6/100    avg_loss:0.21, val_acc:0.91]
Epoch [7/100    avg_loss:0.15, val_acc:0.92]
Epoch [8/100    avg_loss:0.18, val_acc:0.81]
Epoch [9/100    avg_loss:0.17, val_acc:0.92]
Epoch [10/100    avg_loss:0.10, val_acc:0.94]
Epoch [11/100    avg_loss:0.10, val_acc:0.91]
Epoch [12/100    avg_loss:0.07, val_acc:0.93]
Epoch [13/100    avg_loss:0.05, val_acc:0.93]
Epoch [14/100    avg_loss:0.07, val_acc:0.93]
Epoch [15/100    avg_loss:0.07, val_acc:0.94]
Epoch [16/100    avg_loss:0.06, val_acc:0.94]
Epoch [17/100    avg_loss:0.03, val_acc:0.95]
Epoch [18/100    avg_loss:0.02, val_acc:0.95]
Epoch [19/100    avg_loss:0.02, val_acc:0.95]
Epoch [20/100    avg_loss:0.03, val_acc:0.96]
Epoch [21/100    avg_loss:0.02, val_acc:0.96]
Epoch [22/100    avg_loss:0.01, val_acc:0.96]
Epoch [23/100    avg_loss:0.01, val_acc:0.96]
Epoch [24/100    avg_loss:0.01, val_acc:0.95]
Epoch [25/100    avg_loss:0.01, val_acc:0.96]
Epoch [26/100    avg_loss:0.04, val_acc:0.95]
Epoch [27/100    avg_loss:0.04, val_acc:0.95]
Epoch [28/100    avg_loss:0.02, val_acc:0.95]
Epoch [29/100    avg_loss:0.01, val_acc:0.96]
Epoch [30/100    avg_loss:0.01, val_acc:0.95]
Epoch [31/100    avg_loss:0.01, val_acc:0.96]
Epoch [32/100    avg_loss:0.01, val_acc:0.95]
Epoch [33/100    avg_loss:0.01, val_acc:0.97]
Epoch [34/100    avg_loss:0.00, val_acc:0.97]
Epoch [35/100    avg_loss:0.01, val_acc:0.96]
Epoch [36/100    avg_loss:0.03, val_acc:0.96]
Epoch [37/100    avg_loss:0.01, val_acc:0.96]
Epoch [38/100    avg_loss:0.01, val_acc:0.96]
Epoch [39/100    avg_loss:0.00, val_acc:0.97]
Epoch [40/100    avg_loss:0.01, val_acc:0.96]
Epoch [41/100    avg_loss:0.00, val_acc:0.96]
Epoch [42/100    avg_loss:0.00, val_acc:0.96]
Epoch [43/100    avg_loss:0.00, val_acc:0.96]
Epoch [44/100    avg_loss:0.00, val_acc:0.96]
Epoch [45/100    avg_loss:0.00, val_acc:0.96]
Epoch [46/100    avg_loss:0.00, val_acc:0.96]
Epoch [47/100    avg_loss:0.00, val_acc:0.96]
Epoch [48/100    avg_loss:0.00, val_acc:0.97]
Epoch [49/100    avg_loss:0.02, val_acc:0.93]
Epoch [50/100    avg_loss:0.04, val_acc:0.95]
Epoch [51/100    avg_loss:0.02, val_acc:0.94]
Epoch [52/100    avg_loss:0.01, val_acc:0.95]
Epoch [53/100    avg_loss:0.02, val_acc:0.94]
Epoch [54/100    avg_loss:0.01, val_acc:0.96]
Epoch [55/100    avg_loss:0.00, val_acc:0.96]
Epoch [56/100    avg_loss:0.00, val_acc:0.96]
Epoch [57/100    avg_loss:0.00, val_acc:0.96]
Epoch [58/100    avg_loss:0.00, val_acc:0.96]
Epoch [59/100    avg_loss:0.00, val_acc:0.96]
Epoch [60/100    avg_loss:0.00, val_acc:0.97]
Epoch [61/100    avg_loss:0.00, val_acc:0.97]
Epoch [62/100    avg_loss:0.00, val_acc:0.96]
Epoch [63/100    avg_loss:0.00, val_acc:0.97]
Epoch [64/100    avg_loss:0.00, val_acc:0.97]
Epoch [65/100    avg_loss:0.00, val_acc:0.97]
Epoch [66/100    avg_loss:0.00, val_acc:0.96]
Epoch [67/100    avg_loss:0.00, val_acc:0.97]
Epoch [68/100    avg_loss:0.00, val_acc:0.96]
Epoch [69/100    avg_loss:0.00, val_acc:0.96]
Epoch [70/100    avg_loss:0.00, val_acc:0.97]
Epoch [71/100    avg_loss:0.00, val_acc:0.97]
Epoch [72/100    avg_loss:0.00, val_acc:0.96]
Epoch [73/100    avg_loss:0.00, val_acc:0.96]
Epoch [74/100    avg_loss:0.00, val_acc:0.96]
Epoch [75/100    avg_loss:0.00, val_acc:0.96]
Epoch [76/100    avg_loss:0.00, val_acc:0.96]
Epoch [77/100    avg_loss:0.00, val_acc:0.96]
Epoch [78/100    avg_loss:0.00, val_acc:0.96]
Epoch [79/100    avg_loss:0.00, val_acc:0.96]
Epoch [80/100    avg_loss:0.00, val_acc:0.96]
Epoch [81/100    avg_loss:0.00, val_acc:0.97]
Epoch [82/100    avg_loss:0.00, val_acc:0.96]
Epoch [83/100    avg_loss:0.00, val_acc:0.96]
Epoch [84/100    avg_loss:0.01, val_acc:0.97]
Epoch [85/100    avg_loss:0.00, val_acc:0.97]
Epoch [86/100    avg_loss:0.00, val_acc:0.96]
Epoch [87/100    avg_loss:0.01, val_acc:0.97]
Epoch [88/100    avg_loss:0.00, val_acc:0.96]
Epoch [89/100    avg_loss:0.00, val_acc:0.96]
Epoch [90/100    avg_loss:0.00, val_acc:0.96]
Epoch [91/100    avg_loss:0.00, val_acc:0.96]
Epoch [92/100    avg_loss:0.00, val_acc:0.96]
Epoch [93/100    avg_loss:0.00, val_acc:0.96]
Epoch [94/100    avg_loss:0.00, val_acc:0.96]
Epoch [95/100    avg_loss:0.00, val_acc:0.96]
Epoch [96/100    avg_loss:0.00, val_acc:0.96]
Epoch [97/100    avg_loss:0.00, val_acc:0.96]
Epoch [98/100    avg_loss:0.00, val_acc:0.96]
Epoch [99/100    avg_loss:0.00, val_acc:0.97]
Epoch [100/100    avg_loss:0.00, val_acc:0.97]
creating ./logs/logs-2022-06-28IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-06-28:20:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Running an experiment with the Model_by_DMuCA model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_DMuCA model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the Model_by_DMuCA model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
spe_kerner:3
spa_kerner:3
dataset:IndianPines
model:Model_by_DMuCA
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
bands:200
spa_head:16
spe_head:25
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f64a21f5588>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/100    avg_loss:2.16, val_acc:0.64]
Epoch [2/100    avg_loss:1.15, val_acc:0.62]
Epoch [3/100    avg_loss:0.76, val_acc:0.78]
Epoch [4/100    avg_loss:0.50, val_acc:0.82]
Epoch [5/100    avg_loss:0.35, val_acc:0.85]
Epoch [6/100    avg_loss:0.41, val_acc:0.82]
Epoch [7/100    avg_loss:0.29, val_acc:0.90]
Epoch [8/100    avg_loss:0.16, val_acc:0.91]
Epoch [9/100    avg_loss:0.13, val_acc:0.94]
Epoch [10/100    avg_loss:0.09, val_acc:0.94]
Epoch [11/100    avg_loss:0.08, val_acc:0.96]
Epoch [12/100    avg_loss:0.06, val_acc:0.95]
Epoch [13/100    avg_loss:0.04, val_acc:0.96]
Epoch [14/100    avg_loss:0.05, val_acc:0.95]
Epoch [15/100    avg_loss:0.06, val_acc:0.95]
Epoch [16/100    avg_loss:0.03, val_acc:0.96]
Epoch [17/100    avg_loss:0.04, val_acc:0.96]
Epoch [18/100    avg_loss:0.03, val_acc:0.96]
Epoch [19/100    avg_loss:0.03, val_acc:0.96]
Epoch [20/100    avg_loss:0.03, val_acc:0.95]
Epoch [21/100    avg_loss:0.03, val_acc:0.96]
Epoch [22/100    avg_loss:0.02, val_acc:0.96]
Epoch [23/100    avg_loss:0.01, val_acc:0.97]
Epoch [24/100    avg_loss:0.01, val_acc:0.98]
Epoch [25/100    avg_loss:0.01, val_acc:0.97]
Epoch [26/100    avg_loss:0.00, val_acc:0.97]
Epoch [27/100    avg_loss:0.00, val_acc:0.98]
Epoch [28/100    avg_loss:0.01, val_acc:0.97]
Epoch [29/100    avg_loss:0.01, val_acc:0.97]
Epoch [30/100    avg_loss:0.01, val_acc:0.97]
Epoch [31/100    avg_loss:0.01, val_acc:0.97]
Epoch [32/100    avg_loss:0.01, val_acc:0.96]
Epoch [33/100    avg_loss:0.01, val_acc:0.98]
Epoch [34/100    avg_loss:0.00, val_acc:0.97]
Epoch [35/100    avg_loss:0.04, val_acc:0.95]
Epoch [36/100    avg_loss:0.03, val_acc:0.97]
Epoch [37/100    avg_loss:0.01, val_acc:0.97]
Epoch [38/100    avg_loss:0.00, val_acc:0.98]
Epoch [39/100    avg_loss:0.01, val_acc:0.98]
Epoch [40/100    avg_loss:0.00, val_acc:0.97]
Epoch [41/100    avg_loss:0.01, val_acc:0.98]
Epoch [42/100    avg_loss:0.03, val_acc:0.96]
Epoch [43/100    avg_loss:0.01, val_acc:0.97]
Epoch [44/100    avg_loss:0.01, val_acc:0.97]
Epoch [45/100    avg_loss:0.01, val_acc:0.97]
Epoch [46/100    avg_loss:0.00, val_acc:0.97]
Epoch [47/100    avg_loss:0.00, val_acc:0.98]
Epoch [48/100    avg_loss:0.00, val_acc:0.98]
Epoch [49/100    avg_loss:0.00, val_acc:0.98]
Epoch [50/100    avg_loss:0.00, val_acc:0.98]
Epoch [51/100    avg_loss:0.00, val_acc:0.98]
Epoch [52/100    avg_loss:0.00, val_acc:0.97]
Epoch [53/100    avg_loss:0.00, val_acc:0.97]
Epoch [54/100    avg_loss:0.00, val_acc:0.97]
Epoch [55/100    avg_loss:0.00, val_acc:0.97]
Epoch [56/100    avg_loss:0.00, val_acc:0.97]
Epoch [57/100    avg_loss:0.00, val_acc:0.98]
Epoch [58/100    avg_loss:0.00, val_acc:0.98]
Epoch [59/100    avg_loss:0.00, val_acc:0.98]
Epoch [60/100    avg_loss:0.00, val_acc:0.97]
Epoch [61/100    avg_loss:0.00, val_acc:0.97]
Epoch [62/100    avg_loss:0.00, val_acc:0.97]
Epoch [63/100    avg_loss:0.00, val_acc:0.98]
Epoch [64/100    avg_loss:0.00, val_acc:0.97]
Epoch [65/100    avg_loss:0.00, val_acc:0.97]
Epoch [66/100    avg_loss:0.00, val_acc:0.97]
Epoch [67/100    avg_loss:0.00, val_acc:0.97]
Epoch [68/100    avg_loss:0.00, val_acc:0.97]
Epoch [69/100    avg_loss:0.00, val_acc:0.97]
Epoch [70/100    avg_loss:0.00, val_acc:0.97]
Epoch [71/100    avg_loss:0.00, val_acc:0.97]
Epoch [72/100    avg_loss:0.00, val_acc:0.97]
Epoch [73/100    avg_loss:0.00, val_acc:0.97]
Epoch [74/100    avg_loss:0.00, val_acc:0.97]
Epoch [75/100    avg_loss:0.00, val_acc:0.97]
Epoch [76/100    avg_loss:0.00, val_acc:0.98]
Epoch [77/100    avg_loss:0.00, val_acc:0.98]
Epoch [78/100    avg_loss:0.00, val_acc:0.98]
Epoch [79/100    avg_loss:0.00, val_acc:0.98]
Epoch [80/100    avg_loss:0.00, val_acc:0.98]
Epoch [81/100    avg_loss:0.00, val_acc:0.98]
Epoch [82/100    avg_loss:0.00, val_acc:0.98]
Epoch [83/100    avg_loss:0.00, val_acc:0.98]
Epoch [84/100    avg_loss:0.00, val_acc:0.98]
Epoch [85/100    avg_loss:0.00, val_acc:0.98]
Epoch [86/100    avg_loss:0.00, val_acc:0.98]
Epoch [87/100    avg_loss:0.00, val_acc:0.98]
Epoch [88/100    avg_loss:0.00, val_acc:0.98]
Epoch [89/100    avg_loss:0.00, val_acc:0.98]
Epoch [90/100    avg_loss:0.00, val_acc:0.98]
Epoch [91/100    avg_loss:0.00, val_acc:0.98]
Epoch [92/100    avg_loss:0.00, val_acc:0.98]
Epoch [93/100    avg_loss:0.00, val_acc:0.98]
Epoch [94/100    avg_loss:0.00, val_acc:0.98]
Epoch [95/100    avg_loss:0.00, val_acc:0.98]
Epoch [96/100    avg_loss:0.00, val_acc:0.98]
Epoch [97/100    avg_loss:0.00, val_acc:0.98]
Epoch [98/100    avg_loss:0.00, val_acc:0.97]
Epoch [99/100    avg_loss:0.00, val_acc:0.97]
Epoch [100/100    avg_loss:0.00, val_acc:0.97]
creating ./logs/logs-2022-06-28IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-06-28:20:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Running an experiment with the Model_by_COA model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the Model_by_COA model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:Model_by_COA
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
bands:200
spa_head:16
spe_head:25
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8048326518>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/100    avg_loss:2.21, val_acc:0.52]
Epoch [2/100    avg_loss:1.25, val_acc:0.67]
Epoch [3/100    avg_loss:0.83, val_acc:0.78]
Epoch [4/100    avg_loss:0.51, val_acc:0.86]
Epoch [5/100    avg_loss:0.35, val_acc:0.89]
Epoch [6/100    avg_loss:0.25, val_acc:0.92]
Epoch [7/100    avg_loss:0.21, val_acc:0.94]
Epoch [8/100    avg_loss:0.19, val_acc:0.96]
Epoch [9/100    avg_loss:0.16, val_acc:0.95]
Epoch [10/100    avg_loss:0.11, val_acc:0.95]
Epoch [11/100    avg_loss:0.08, val_acc:0.96]
Epoch [12/100    avg_loss:0.06, val_acc:0.97]
Epoch [13/100    avg_loss:0.05, val_acc:0.97]
Epoch [14/100    avg_loss:0.06, val_acc:0.98]
Epoch [15/100    avg_loss:0.03, val_acc:0.96]
Epoch [16/100    avg_loss:0.04, val_acc:0.97]
Epoch [17/100    avg_loss:0.05, val_acc:0.94]
Epoch [18/100    avg_loss:0.07, val_acc:0.97]
Epoch [19/100    avg_loss:0.05, val_acc:0.97]
Epoch [20/100    avg_loss:0.04, val_acc:0.97]
Epoch [21/100    avg_loss:0.03, val_acc:0.98]
Epoch [22/100    avg_loss:0.01, val_acc:0.98]
Epoch [23/100    avg_loss:0.01, val_acc:0.98]
Epoch [24/100    avg_loss:0.01, val_acc:0.98]
Epoch [25/100    avg_loss:0.05, val_acc:0.96]
Epoch [26/100    avg_loss:0.03, val_acc:0.98]
Epoch [27/100    avg_loss:0.02, val_acc:0.98]
Epoch [28/100    avg_loss:0.01, val_acc:0.98]
Epoch [29/100    avg_loss:0.01, val_acc:0.98]
Epoch [30/100    avg_loss:0.01, val_acc:0.98]
Epoch [31/100    avg_loss:0.03, val_acc:0.98]
Epoch [32/100    avg_loss:0.01, val_acc:0.98]
Epoch [33/100    avg_loss:0.01, val_acc:0.98]
Epoch [34/100    avg_loss:0.01, val_acc:0.98]
Epoch [35/100    avg_loss:0.04, val_acc:0.98]
Epoch [36/100    avg_loss:0.01, val_acc:0.97]
Epoch [37/100    avg_loss:0.01, val_acc:0.97]
Epoch [38/100    avg_loss:0.04, val_acc:0.98]
Epoch [39/100    avg_loss:0.01, val_acc:0.98]
Epoch [40/100    avg_loss:0.01, val_acc:0.97]
Epoch [41/100    avg_loss:0.01, val_acc:0.98]
Epoch [42/100    avg_loss:0.01, val_acc:0.99]
Epoch [43/100    avg_loss:0.01, val_acc:0.98]
Epoch [44/100    avg_loss:0.00, val_acc:0.98]
Epoch [45/100    avg_loss:0.00, val_acc:0.98]
Epoch [46/100    avg_loss:0.01, val_acc:0.98]
Epoch [47/100    avg_loss:0.00, val_acc:0.98]
Epoch [48/100    avg_loss:0.01, val_acc:0.99]
Epoch [49/100    avg_loss:0.00, val_acc:0.99]
Epoch [50/100    avg_loss:0.01, val_acc:0.98]
Epoch [51/100    avg_loss:0.00, val_acc:0.98]
Epoch [52/100    avg_loss:0.00, val_acc:0.99]
Epoch [53/100    avg_loss:0.00, val_acc:0.98]
Epoch [54/100    avg_loss:0.00, val_acc:0.99]
Epoch [55/100    avg_loss:0.00, val_acc:0.99]
Epoch [56/100    avg_loss:0.00, val_acc:0.99]
Epoch [57/100    avg_loss:0.00, val_acc:0.99]
Epoch [58/100    avg_loss:0.00, val_acc:0.98]
Epoch [59/100    avg_loss:0.00, val_acc:0.98]
Epoch [60/100    avg_loss:0.00, val_acc:0.99]
Epoch [61/100    avg_loss:0.00, val_acc:0.98]
Epoch [62/100    avg_loss:0.00, val_acc:0.99]
Epoch [63/100    avg_loss:0.00, val_acc:0.98]
Epoch [64/100    avg_loss:0.00, val_acc:0.98]
Epoch [65/100    avg_loss:0.00, val_acc:0.99]
Epoch [66/100    avg_loss:0.00, val_acc:0.99]
Epoch [67/100    avg_loss:0.00, val_acc:0.99]
Epoch [68/100    avg_loss:0.00, val_acc:0.99]
Epoch [69/100    avg_loss:0.00, val_acc:0.99]
Epoch [70/100    avg_loss:0.00, val_acc:0.99]
Epoch [71/100    avg_loss:0.00, val_acc:0.99]
Epoch [72/100    avg_loss:0.00, val_acc:0.99]
Epoch [73/100    avg_loss:0.00, val_acc:0.98]
Epoch [74/100    avg_loss:0.05, val_acc:0.96]
Epoch [75/100    avg_loss:0.03, val_acc:0.97]
Epoch [76/100    avg_loss:0.03, val_acc:0.97]
Epoch [77/100    avg_loss:0.01, val_acc:0.98]
Epoch [78/100    avg_loss:0.01, val_acc:0.95]
Epoch [79/100    avg_loss:0.02, val_acc:0.98]
Epoch [80/100    avg_loss:0.01, val_acc:0.98]
Epoch [81/100    avg_loss:0.01, val_acc:0.99]
Epoch [82/100    avg_loss:0.01, val_acc:0.97]
Epoch [83/100    avg_loss:0.03, val_acc:0.98]
Epoch [84/100    avg_loss:0.03, val_acc:0.98]
Epoch [85/100    avg_loss:0.01, val_acc:0.98]
Epoch [86/100    avg_loss:0.01, val_acc:0.98]
Epoch [87/100    avg_loss:0.02, val_acc:0.92]
Epoch [88/100    avg_loss:0.04, val_acc:0.98]
Epoch [89/100    avg_loss:0.03, val_acc:0.97]
Epoch [90/100    avg_loss:0.01, val_acc:0.98]
Epoch [91/100    avg_loss:0.01, val_acc:0.98]
Epoch [92/100    avg_loss:0.02, val_acc:0.98]
Epoch [93/100    avg_loss:0.00, val_acc:0.99]
Epoch [94/100    avg_loss:0.00, val_acc:0.99]
Epoch [95/100    avg_loss:0.00, val_acc:0.99]
Epoch [96/100    avg_loss:0.00, val_acc:0.99]
Epoch [97/100    avg_loss:0.00, val_acc:0.99]
Epoch [98/100    avg_loss:0.00, val_acc:0.99]
Epoch [99/100    avg_loss:0.00, val_acc:0.99]
Epoch [100/100    avg_loss:0.00, val_acc:0.99]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1267    2    1    0    6    0    0    0    0    7    2    0
     0    0    0]
 [   0    0    1  733   12    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0    0   10  203    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  421    0    4    0    0    0    3    0    0
     5    2    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    1    0    0   16    0    0    0    0
     0    0    0]
 [   0    1    8    0    0    0    0    0    0    0  840   15    8    0
     0    3    0]
 [   0    0    5    0    0    0    1    0    0    0    7 2187    8    0
     0    2    0]
 [   0    0    0    3    1    6    0    0    0    0    3    0  514    0
     0    6    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1129   10    0]
 [   0    0    0    0    0    0   50    0    0    0    0    0    0    0
    43  254    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.3442

F1 scores:
[   nan 0.988  0.9875 0.9799 0.9442 0.9768 0.9547 0.9259 1.     0.9412
 0.9733 0.9887 0.9625 1.     0.9741 0.8141 0.982 ]

Kappa:
0.9697
dataset:IndianPines
['98.8+-0.0' '98.75+-0.0' '97.99+-0.0' '94.42+-0.0' '97.68+-0.0'
 '95.47+-0.0' '92.59+-0.0' '100.0+-0.0' '94.12+-0.0' '97.33+-0.0'
 '98.87+-0.0' '96.25+-0.0' '100.0+-0.0' '97.41+-0.0' '81.41+-0.0'
 '98.2+-0.0']
OA_list [[97.34417344]]
OA±std 97.34 ±0.00
creating ./logs/logs-2022-06-28IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-06-28:20:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Running an experiment with the Model_by_COA model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the Model_by_COA model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the Model_by_COA model
Train dataloader:30
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:Model_by_COA
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
test_stride:1
n_classes:17
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
bands:200
spa_head:16
spe_head:25
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd4a92e34e0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/100    avg_loss:2.44, val_acc:0.59]
Epoch [2/100    avg_loss:0.96, val_acc:0.77]
Epoch [3/100    avg_loss:0.76, val_acc:0.78]
Epoch [4/100    avg_loss:0.49, val_acc:0.83]
Epoch [5/100    avg_loss:0.47, val_acc:0.85]
Epoch [6/100    avg_loss:0.26, val_acc:0.90]
Epoch [7/100    avg_loss:0.23, val_acc:0.89]
Epoch [8/100    avg_loss:0.17, val_acc:0.93]
Epoch [9/100    avg_loss:0.12, val_acc:0.96]
Epoch [10/100    avg_loss:0.06, val_acc:0.96]
Epoch [11/100    avg_loss:0.07, val_acc:0.97]
Epoch [12/100    avg_loss:0.05, val_acc:0.97]
Epoch [13/100    avg_loss:0.04, val_acc:0.96]
Epoch [14/100    avg_loss:0.07, val_acc:0.96]
Epoch [15/100    avg_loss:0.08, val_acc:0.97]
Epoch [16/100    avg_loss:0.10, val_acc:0.93]
Epoch [17/100    avg_loss:0.07, val_acc:0.94]
Epoch [18/100    avg_loss:0.07, val_acc:0.96]
Epoch [19/100    avg_loss:0.03, val_acc:0.96]
Epoch [20/100    avg_loss:0.02, val_acc:0.96]
Epoch [21/100    avg_loss:0.02, val_acc:0.97]
Epoch [22/100    avg_loss:0.04, val_acc:0.96]
Epoch [23/100    avg_loss:0.04, val_acc:0.98]
Epoch [24/100    avg_loss:0.03, val_acc:0.97]
Epoch [25/100    avg_loss:0.02, val_acc:0.98]
Epoch [26/100    avg_loss:0.01, val_acc:0.98]
Epoch [27/100    avg_loss:0.01, val_acc:0.98]
Epoch [28/100    avg_loss:0.01, val_acc:0.98]
Epoch [29/100    avg_loss:0.01, val_acc:0.98]
Epoch [30/100    avg_loss:0.01, val_acc:0.98]
Epoch [31/100    avg_loss:0.00, val_acc:0.98]
Epoch [32/100    avg_loss:0.01, val_acc:0.98]
Epoch [33/100    avg_loss:0.01, val_acc:0.98]
Epoch [34/100    avg_loss:0.01, val_acc:0.98]
Epoch [35/100    avg_loss:0.01, val_acc:0.98]
Epoch [36/100    avg_loss:0.01, val_acc:0.98]
Epoch [37/100    avg_loss:0.01, val_acc:0.98]
Epoch [38/100    avg_loss:0.01, val_acc:0.98]
Epoch [39/100    avg_loss:0.01, val_acc:0.98]
Epoch [40/100    avg_loss:0.00, val_acc:0.98]
Epoch [41/100    avg_loss:0.00, val_acc:0.98]
Epoch [42/100    avg_loss:0.00, val_acc:0.98]
Epoch [43/100    avg_loss:0.00, val_acc:0.98]
Epoch [44/100    avg_loss:0.00, val_acc:0.98]
Epoch [45/100    avg_loss:0.00, val_acc:0.98]
Epoch [46/100    avg_loss:0.00, val_acc:0.98]
Epoch [47/100    avg_loss:0.00, val_acc:0.98]
Epoch [48/100    avg_loss:0.00, val_acc:0.98]
Epoch [49/100    avg_loss:0.00, val_acc:0.98]
Epoch [50/100    avg_loss:0.00, val_acc:0.98]
Epoch [51/100    avg_loss:0.00, val_acc:0.98]
Epoch [52/100    avg_loss:0.00, val_acc:0.98]
Epoch [53/100    avg_loss:0.00, val_acc:0.98]
Epoch [54/100    avg_loss:0.00, val_acc:0.98]
Epoch [55/100    avg_loss:0.00, val_acc:0.98]
Epoch [56/100    avg_loss:0.00, val_acc:0.98]
Epoch [57/100    avg_loss:0.00, val_acc:0.98]
Epoch [58/100    avg_loss:0.00, val_acc:0.98]
Epoch [59/100    avg_loss:0.01, val_acc:0.97]
Epoch [60/100    avg_loss:0.02, val_acc:0.98]
Epoch [61/100    avg_loss:0.01, val_acc:0.98]
Epoch [62/100    avg_loss:0.01, val_acc:0.98]
Epoch [63/100    avg_loss:0.07, val_acc:0.96]
Epoch [64/100    avg_loss:0.14, val_acc:0.95]
Epoch [65/100    avg_loss:0.17, val_acc:0.92]
Epoch [66/100    avg_loss:0.08, val_acc:0.95]
Epoch [67/100    avg_loss:0.05, val_acc:0.98]
Epoch [68/100    avg_loss:0.06, val_acc:0.97]
Epoch [69/100    avg_loss:0.04, val_acc:0.97]
Epoch [70/100    avg_loss:0.04, val_acc:0.97]
Epoch [71/100    avg_loss:0.01, val_acc:0.98]
Epoch [72/100    avg_loss:0.01, val_acc:0.98]
Epoch [73/100    avg_loss:0.01, val_acc:0.98]
Epoch [74/100    avg_loss:0.01, val_acc:0.98]
Epoch [75/100    avg_loss:0.02, val_acc:0.98]
Epoch [76/100    avg_loss:0.01, val_acc:0.97]
Epoch [77/100    avg_loss:0.01, val_acc:0.97]
Epoch [78/100    avg_loss:0.01, val_acc:0.98]
Epoch [79/100    avg_loss:0.00, val_acc:0.98]
Epoch [80/100    avg_loss:0.00, val_acc:0.98]
Epoch [81/100    avg_loss:0.00, val_acc:0.98]
Epoch [82/100    avg_loss:0.02, val_acc:0.97]
Epoch [83/100    avg_loss:0.01, val_acc:0.97]
Epoch [84/100    avg_loss:0.02, val_acc:0.98]
Epoch [85/100    avg_loss:0.00, val_acc:0.98]
Epoch [86/100    avg_loss:0.01, val_acc:0.98]
Epoch [87/100    avg_loss:0.00, val_acc:0.98]
Epoch [88/100    avg_loss:0.00, val_acc:0.98]
Epoch [89/100    avg_loss:0.00, val_acc:0.99]
Epoch [90/100    avg_loss:0.00, val_acc:0.98]
Epoch [91/100    avg_loss:0.00, val_acc:0.98]
Epoch [92/100    avg_loss:0.00, val_acc:0.98]
Epoch [93/100    avg_loss:0.00, val_acc:0.98]
Epoch [94/100    avg_loss:0.00, val_acc:0.98]
Epoch [95/100    avg_loss:0.00, val_acc:0.98]
Epoch [96/100    avg_loss:0.00, val_acc:0.99]
Epoch [97/100    avg_loss:0.00, val_acc:0.98]
Epoch [98/100    avg_loss:0.00, val_acc:0.98]
Epoch [99/100    avg_loss:0.00, val_acc:0.98]
Epoch [100/100    avg_loss:0.00, val_acc:0.99]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1270    1    0    0    1    0    0    0    2    7    4    0
     0    0    0]
 [   0    0    7  739    1    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    6  207    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  647    0    0    0    0   10    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0   13    0    0    0    0    0    0    0  856    6    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0    0 2205    0    0
     0    0    0]
 [   0    0    0    9    0    3    0    0    0    0   10   13  497    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0    0    0    0    1
  1137    0    0]
 [   0    0    0    0    0    0   43    0    0    0    0    0    0    0
    88  216    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0   13    0
     0    0   71]]

Accuracy:
97.2683

F1 scores:
[   nan 0.975  0.9834 0.9834 0.9834 0.9931 0.9599 1.     1.     0.9714
 0.9822 0.9908 0.9485 0.9973 0.9607 0.766  0.9103]

Kappa:
0.9688
dataset:IndianPines
['97.5+-0.0' '98.34+-0.0' '98.34+-0.0' '98.34+-0.0' '99.31+-0.0'
 '95.99+-0.0' '100.0+-0.0' '100.0+-0.0' '97.14+-0.0' '98.22+-0.0'
 '99.08+-0.0' '94.85+-0.0' '99.73+-0.0' '96.07+-0.0' '76.6+-0.0'
 '91.03+-0.0']
OA_list [[97.26829268]]
OA±std 97.27 ±0.00
